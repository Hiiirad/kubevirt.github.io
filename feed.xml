<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.1">Jekyll</generator><link href="https://kubevirt.io//feed.xml" rel="self" type="application/atom+xml" /><link href="https://kubevirt.io//" rel="alternate" type="text/html" /><updated>2021-10-11T22:25:46+00:00</updated><id>https://kubevirt.io//feed.xml</id><title type="html">KubeVirt.io</title><subtitle>Virtual Machine Management on Kubernetes</subtitle><entry><title type="html">KubeVirt v0.46.0</title><link href="https://kubevirt.io//2021/changelog-v0.46.0.html" rel="alternate" type="text/html" title="KubeVirt v0.46.0" /><published>2021-10-08T00:00:00+00:00</published><updated>2021-10-08T00:00:00+00:00</updated><id>https://kubevirt.io//2021/changelog-v0.46.0</id><content type="html" xml:base="https://kubevirt.io//2021/changelog-v0.46.0.html">&lt;h2 id=&quot;v0460&quot;&gt;v0.46.0&lt;/h2&gt;

&lt;p&gt;Released on: Fri Oct 8 21:12:33 2021 +0000&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;[PR #6425][awels] Hotplug disks are possible when iothreads are enabled.&lt;/li&gt;
  &lt;li&gt;[PR #6297][acardace] mutate migration PDBs instead of creating an additional one for the duration of the migration.&lt;/li&gt;
  &lt;li&gt;[PR #6464][awels] BugFix: Fixed hotplug race between kubelet and virt-handler when virt-launcher dies unexpectedly.&lt;/li&gt;
  &lt;li&gt;[PR #6465][salanki] Fix corrupted DHCP Gateway Option from local DHCP server, leading to rejected IP configuration on Windows VMs.&lt;/li&gt;
  &lt;li&gt;[PR #6458][vladikr] Tagged SR-IOV interfaces will now appear in the config drive metadata&lt;/li&gt;
  &lt;li&gt;[PR #6446][brybacki] Access mode for virtctl image upload is now optional. This version of virtctl now requires CDI v1.34 or greater&lt;/li&gt;
  &lt;li&gt;[PR #6391][zcahana] Cleanup obsolete permissions from virt-operator‚Äôs ClusterRole&lt;/li&gt;
  &lt;li&gt;[PR #6419][rthallisey] Fix virt-controller panic caused by lots of deleted VMI events&lt;/li&gt;
  &lt;li&gt;[PR #5972][kwiesmueller] Add a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ssh&lt;/code&gt; command to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;virtctl&lt;/code&gt; that can be used to open SSH sessions to VMs/VMIs.&lt;/li&gt;
  &lt;li&gt;[PR #6403][jrife] Removed go module pinning to an old version (v0.3.0) of github.com/go-kit/kit&lt;/li&gt;
  &lt;li&gt;[PR #6367][brybacki] virtctl imageupload now uses DataVolume.spec.storage&lt;/li&gt;
  &lt;li&gt;[PR #6198][iholder-redhat] Fire a Prometheus alert when a lot of REST failures are detected in virt-api&lt;/li&gt;
  &lt;li&gt;[PR #6211][davidvossel] cluster-profiler pprof gathering tool and corresponding ‚ÄúClusterProfiler‚Äù feature gate&lt;/li&gt;
  &lt;li&gt;[PR #6323][vladikr] switch live migration to use unix sockets&lt;/li&gt;
  &lt;li&gt;[PR #6374][vladikr] Fix the default setting of CPU requests on vmipods&lt;/li&gt;
  &lt;li&gt;[PR #6283][rthallisey] Record the time it takes to delete a VMI and expose it as a metric&lt;/li&gt;
  &lt;li&gt;[PR #6251][rmohr] Better place vcpu threads on host cpus to form more efficient passthrough architectures&lt;/li&gt;
  &lt;li&gt;[PR #6377][rmohr] Don‚Äôt fail on failed selinux relabel attempts if selinux is permissive&lt;/li&gt;
  &lt;li&gt;[PR #6308][awels] BugFix: hotplug was broken when using it with a hostpath volume that was on a separate device.&lt;/li&gt;
  &lt;li&gt;[PR #6186][davidvossel] Add resource and verb labels to rest_client_requests_total metric&lt;/li&gt;
&lt;/ul&gt;</content><author><name>kubeü§ñ</name></author><category term="releases" /><category term="release notes" /><category term="changelog" /><summary type="html">v0.46.0</summary></entry><entry><title type="html">Import AWS AMIs as KubeVirt Golden Images</title><link href="https://kubevirt.io//2021/Importing-EC2-to-KubeVirt.html" rel="alternate" type="text/html" title="Import AWS AMIs as KubeVirt Golden Images" /><published>2021-09-21T00:00:00+00:00</published><updated>2021-09-21T00:00:00+00:00</updated><id>https://kubevirt.io//2021/Importing-EC2-to-KubeVirt</id><content type="html" xml:base="https://kubevirt.io//2021/Importing-EC2-to-KubeVirt.html">&lt;h2 id=&quot;breaking-out&quot;&gt;Breaking Out&lt;/h2&gt;

&lt;p&gt;There comes a point where an operations team has invested so heavily in a Iaas platform that they are effectively locked into that platform. For example, here‚Äôs one scenario outlining how this can happen. An operations team has created automation around building VM images and keeping images up-to-date. In AWS that automation likely involves starting an EC2 instance, injecting some application logic into that instance, sealing the instance‚Äôs boot source as an AMI, and finally copying that AMI around to all the AWS regions the team deploys in.&lt;/p&gt;

&lt;p&gt;If the team was interested in evaluating KubeVirt as an alternative Iaas platform to AWS‚Äôs EC2, given the team‚Äôs existing tooling there‚Äôs not a clear path for doing this. It‚Äôs that scenario where the tooling in the &lt;a href=&quot;https://github.com/davidvossel/kubevirt-cloud-import&quot;&gt;kubevirt-cloud-import&lt;/a&gt; project comes into play.&lt;/p&gt;

&lt;h2 id=&quot;kubevirt-cloud-import&quot;&gt;Kubevirt Cloud Import&lt;/h2&gt;

&lt;p&gt;The &lt;a href=&quot;https://github.com/davidvossel/kubevirt-cloud-import&quot;&gt;KubeVirt Cloud Import&lt;/a&gt; project explores the practicality of transitioning VMs from various cloud providers into KubeVirt. As of writing this, automation for exporting AMIs from EC2 into KubeVirt works, and it‚Äôs really not all that complicated.&lt;/p&gt;

&lt;p&gt;This blog post will explore the fundamentals of how AMIs are exported, and how the KubeVirt Cloud Import project leverages these techniques to build automation pipelines.&lt;/p&gt;

&lt;h2 id=&quot;nuts-and-bolts-of-importing-amis&quot;&gt;Nuts and Bolts of Importing AMIs&lt;/h2&gt;

&lt;h3 id=&quot;official-aws-ami-export-support&quot;&gt;Official AWS AMI Export Support&lt;/h3&gt;

&lt;p&gt;AWS supports an &lt;a href=&quot;https://docs.aws.amazon.com/vm-import/latest/userguide/vmexport_image.html&quot;&gt;api&lt;/a&gt; for exporting AMIs as a file to an s3 bucket. This support works quite well, however there‚Äôs a long list of &lt;a href=&quot;https://docs.aws.amazon.com/vm-import/latest/userguide/vmexport_image.html#limits-image-export&quot;&gt;limitations&lt;/a&gt; that impact what AMIs are eligible for export. The most limiting of those items is the one that prevents any image built from an AMI on the marketplace from being eligible for the official export support.&lt;/p&gt;

&lt;h3 id=&quot;unofficial-aws-export-support&quot;&gt;Unofficial AWS export Support&lt;/h3&gt;

&lt;p&gt;Regardless of what AWS officially supports or not, there‚Äôs absolutely nothing preventing someone from exporting an AMI‚Äôs contents themselves. The technique just involves creating an EC2 instance, attaching an EBS volume (containing the AMI contents) as a block device, then streaming that block devices contents where ever you want.&lt;/p&gt;

&lt;p&gt;Theoretically, the steps roughly look like this.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Convert AMI to a volume by finding the underlying AMI‚Äôs snapshot and converting it to an EBS volume.&lt;/li&gt;
  &lt;li&gt;Create an EC2 instance with the EBS volume containing the AMI contents as a secondary data device.&lt;/li&gt;
  &lt;li&gt;Within the EC2 guest, copy the EBS device‚Äôs contents as a disk img &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;dd if=/dev/xvda of=/tmp/disk/disk.img&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Then upload the disk image to an object store like s3. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;aws s3 cp /tmp/disk/disk.img s3://my-b1-bucket/ upload: ../tmp/disk/disk.img to s3://my-b1-bucket/disk.img&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;basics-of-importing-data-into-kubevirt&quot;&gt;Basics of Importing Data into KubeVirt&lt;/h3&gt;

&lt;p&gt;Once a disk image is in s3, a KubeVirt companion project called the &lt;a href=&quot;https://github.com/kubevirt/containerized-data-importer&quot;&gt;Containerized Data Importer&lt;/a&gt; (or CDI for short) can be used to import the disk from s3 into a PVC within the KubeVirt cluster. This import flow can be expressed as a CDI DataVolume custom resource.&lt;/p&gt;

&lt;p&gt;Below is an example yaml for importing s3 contents into a PVC using a DataVolume&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;na&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;cdi.kubevirt.io/v1beta1&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;DataVolume&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;example-import-dv&quot;&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;source&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;s3&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
         &lt;span class=&quot;na&quot;&gt;url&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;https://s3.us-west-2.amazonaws.com/my-ami-exports/kubevirt-image-exports/export-ami-0dc4e69702f74df50.vmdk&quot;&lt;/span&gt;
         &lt;span class=&quot;na&quot;&gt;secretRef&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;my-s3-credentials&quot;&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;pvc&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;accessModes&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;ReadWriteOnce&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;resources&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;requests&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;storage&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;6Gi&quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Once the AMI file content is stored in a PVC, CDI can be used further to clone that AMI‚Äôs PVC on a per VM basis. This effectively recreates the AMI to EC2 relationship that exists in AWS. You can find more information about CDI &lt;a href=&quot;https://github.com/kubevirt/containerized-data-importer&quot;&gt;here&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;automating-ami-import&quot;&gt;Automating AMI import&lt;/h2&gt;

&lt;p&gt;Using the technique of exporting an AMI to an s3 bucket and importing the AMI from s3 into a KubeVirt cluster using CDI, the Kubevirt Cloud Import project provides the glue necessary for tying all of these pieces together in the form of the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;import-ami&lt;/code&gt; cli command and a Tekton task.&lt;/p&gt;

&lt;h2 id=&quot;automation-using-the-import-ami-cli-command&quot;&gt;Automation using the import-ami CLI command&lt;/h2&gt;

&lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;import-ami&lt;/code&gt; takes a set of arguments related to the AMI you wish to import into KubeVirt and the name of the PVC you‚Äôd like the AMI to be imported into. Upon execution, import-ami will call all the appropriate AWS and KubeVirt APIs to make this work. The result is a PVC with the AMI contents that is capable of being launched by a KubeVirt VM.&lt;/p&gt;

&lt;p&gt;In the example below, A publicly shared &lt;a href=&quot;https://alt.fedoraproject.org/cloud/&quot;&gt;fedora34 AMI&lt;/a&gt; is imported into the KubeVirt cluster as a PVC called fedora34-golden-image&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
&lt;span class=&quot;nb&quot;&gt;export &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;S3_BUCKET&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;my-bucket
&lt;span class=&quot;nb&quot;&gt;export &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;S3_SECRET&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;s3-readonly-cred
&lt;span class=&quot;nb&quot;&gt;export &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;AWS_REGION&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;us-west-2
&lt;span class=&quot;nb&quot;&gt;export &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;AMI_ID&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;ami-00a4fdd3db8bb2851
&lt;span class=&quot;nb&quot;&gt;export &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;PVC_STORAGECLASS&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;rook-ceph-block
&lt;span class=&quot;nb&quot;&gt;export &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;PVC_NAME&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;fedora34-golden-image

import-ami &lt;span class=&quot;nt&quot;&gt;--s3-bucket&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$S3_BUCKET&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--region&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$AWS_REGION&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--ami-id&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$AMI_ID&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--pvc-storageclass&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$PVC_STORAGECLASS&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--s3-secret&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$S3_SECRET&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--pvc-name&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$PVC_NAME&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;automation-using-the-import-ami-tekton-task&quot;&gt;Automation using the import-ami Tekton Task&lt;/h2&gt;

&lt;p&gt;In addition to the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;import-ami&lt;/code&gt; cli command, the KubeVirt Cloud Import project also includes a &lt;a href=&quot;https://github.com/davidvossel/kubevirt-cloud-import/blob/main/tasks/import-ami/manifests/import-ami.yaml&quot;&gt;Tekton task&lt;/a&gt; which wraps the cli command and allows integrating AMI import into a Tekton pipeline.&lt;/p&gt;

&lt;p&gt;Using a Tekton pipeline, someone can combine the task of importing an AMI into KubeVirt with the task of starting a VM using that AMI. An example pipeline can be found &lt;a href=&quot;https://raw.githubusercontent.com/davidvossel/kubevirt-cloud-import/main/examples/create-vm-from-ami-pipeline.yaml&quot;&gt;here&lt;/a&gt; which outlines how this is accomplished.&lt;/p&gt;

&lt;p&gt;Below is a pipeline run that uses the example pipeline to import the publicly shared fedora34 AMI into a PVC, then starts a VM using that imported AMI.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
&lt;span class=&quot;nb&quot;&gt;cat&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;EOF&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt; &amp;gt; pipeline-run.yaml
apiVersion: tekton.dev/v1beta1
kind: PipelineRun
metadata:
  name: my-vm-creation-pipeline
  namespace: default
spec:
  serviceAccountName: my-kubevirt-service-account
  pipelineRef:
    name: create-vm-pipeline 
  params:
    - name: vmName
      value: vm-fedora34
    - name: s3Bucket
      value: my-kubevirt-exports
    - name: s3ReadCredentialsSecret
      value: my-s3-read-only-credentials
    - name: awsRegion
      value: us-west-2
    - name: amiId 
      value: ami-00a4fdd3db8bb2851
    - name: pvcStorageClass 
      value: rook-ceph-block
    - name: pvcName
      value: fedora34
    - name: pvcNamespace
      value: default
    - name: pvcSize
      value: 6Gi
    - name: pvcAccessMode
      value: ReadWriteOnce
    - name: awsCredentialsSecret
      value: my-aws-credentials
&lt;/span&gt;&lt;span class=&quot;no&quot;&gt;EOF

&lt;/span&gt;kubectl create &lt;span class=&quot;nt&quot;&gt;-f&lt;/span&gt; pipeline-run.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;After posting the pipeline run, watch for the pipeline run to complete.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl get pipelinerun
selecting docker as container runtime
NAME                      SUCCEEDED   REASON      STARTTIME   COMPLETIONTIME
my-vm-creation-pipeline   True        Succeeded   11m         9m54s
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Then observe that the resulting VM is online&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl get vmi
selecting docker as container runtime
NAME          AGE   PHASE     IP               NODENAME   READY
vm-fedora34   11m   Running   10.244.196.175   node01     True
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;For more detailed and up-to-date information about how to automate AMI import using Tekton, view the KubeVirt Cloud Import &lt;a href=&quot;https://github.com/davidvossel/kubevirt-cloud-import/blob/main/README.md&quot;&gt;README.md&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;key-takeaways&quot;&gt;Key Takeaways&lt;/h2&gt;

&lt;p&gt;The portability of workloads across different environments is becoming increasingly important and operations teams need to be vigilant about avoiding vendor lock in. For containers, Kubernetes is an attractive option because it provides a consistent API layer that can run across multiple cloud platforms. KubeVirt can provide that same level of consistency for VMs. As a community we need to invest further into automation tools that allow people to make the transition to KubeVirt.&lt;/p&gt;</content><author><name>David Vossel</name></author><category term="news" /><category term="kubevirt" /><category term="kubernetes" /><category term="virtual machine" /><category term="VM" /><category term="AWS" /><category term="EC2" /><category term="AMI" /><summary type="html">Breaking Out</summary></entry><entry><title type="html">KubeVirt v0.45.0</title><link href="https://kubevirt.io//2021/changelog-v0.45.0.html" rel="alternate" type="text/html" title="KubeVirt v0.45.0" /><published>2021-09-08T00:00:00+00:00</published><updated>2021-09-08T00:00:00+00:00</updated><id>https://kubevirt.io//2021/changelog-v0.45.0</id><content type="html" xml:base="https://kubevirt.io//2021/changelog-v0.45.0.html">&lt;h2 id=&quot;v0450&quot;&gt;v0.45.0&lt;/h2&gt;

&lt;p&gt;Released on: Wed Sep 8 13:56:47 2021 +0000&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;[PR #6191][marceloamaral] Addition of perfscale-load-generator to perform stress tests to evaluate the control plane&lt;/li&gt;
  &lt;li&gt;[PR #6248][VirrageS] Reduced logging in hot paths&lt;/li&gt;
  &lt;li&gt;[PR #6079][weihanglo] Hotplug volume can be unplugged at anytime and reattached after a VM restart.&lt;/li&gt;
  &lt;li&gt;[PR #6101][rmohr] Make k8s client rate limits configurable&lt;/li&gt;
  &lt;li&gt;[PR #6204][sradco] This PR adds to each alert the runbook url that points to a runbook that provides additional details on each alert and how to mitigate it.&lt;/li&gt;
  &lt;li&gt;[PR #5974][vladikr] a list of desired mdev types can now be provided in KubeVirt CR to kubevirt to configure these devices on relevant nodes&lt;/li&gt;
  &lt;li&gt;[PR #6147][rmohr] Fix rbac permissions for freeze/unfreeze, addvolume/removevolume, guestosinfo, filesystemlist and userlist&lt;/li&gt;
  &lt;li&gt;[PR #6161][ashleyschuett] Remove HostDevice validation on VMI creation&lt;/li&gt;
  &lt;li&gt;[PR #6078][zcahana] Report ErrImagePull/ImagePullBackOff VM status when image pull errors occur&lt;/li&gt;
  &lt;li&gt;[PR #6176][kwiesmueller] Fix goroutine leak in virt-handler, potentially causing issues with a high turnover of VMIs.&lt;/li&gt;
  &lt;li&gt;[PR #6047][ShellyKa13] Add phases to the vm snapshot api, specifically a failure phase&lt;/li&gt;
  &lt;li&gt;[PR #6138][ansijain] NA&lt;/li&gt;
&lt;/ul&gt;</content><author><name>kubeü§ñ</name></author><category term="releases" /><category term="release notes" /><category term="changelog" /><summary type="html">v0.45.0</summary></entry><entry><title type="html">Running virtual machines in Istio service mesh</title><link href="https://kubevirt.io//2021/Virtual-machines-in-Istio-service-mesh.html" rel="alternate" type="text/html" title="Running virtual machines in Istio service mesh" /><published>2021-08-13T00:00:00+00:00</published><updated>2021-08-13T00:00:00+00:00</updated><id>https://kubevirt.io//2021/Virtual-machines-in-Istio-service-mesh</id><content type="html" xml:base="https://kubevirt.io//2021/Virtual-machines-in-Istio-service-mesh.html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;This blog post demonstrates running virtual machines in &lt;a href=&quot;https://istio.io/&quot;&gt;Istio&lt;/a&gt; service mesh.&lt;/p&gt;

&lt;p&gt;Istio service mesh allows to monitor, visualize, and manage traffic between pods and external services by
injecting a proxy container - a sidecar - which forwards inbound and outbound traffic of a pod/virtual machine.
This allows the sidecar to collect metadata about the proxied traffic and also actively interfere with it. For more in-depth information about the Istio proxy mechanism, see &lt;a href=&quot;https://medium.com/open-5g-hypercore/episode-iii-meshville-7f0bb7ca0e3b&quot;&gt;this blog post&lt;/a&gt; published by Dough Smith et al.&lt;/p&gt;

&lt;p&gt;The main features of Istio are traffic shifting (migrating traffic from an old to new version of a service), dynamic request routing, fault injection or traffic mirroring for testing/debugging purposes, and more.
Visit &lt;a href=&quot;https://istio.io/latest/docs/tasks/&quot;&gt;Istio documentation&lt;/a&gt; to learn about all its features.
Istio featureset may be further extended by installing addons. Kiali, for example, is a UI dashboard that provides traffic information
of all microservices in a mesh, capable of composing communication graph between all microservices.&lt;/p&gt;

&lt;h2 id=&quot;prerequisites&quot;&gt;Prerequisites&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;KubeVirt &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;v0.43.0&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;CRI-O &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;v1.19.0&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;limitations&quot;&gt;Limitations&lt;/h2&gt;

&lt;p&gt;Istio is only supported with masquerade network binding and pod network over IPv4.&lt;/p&gt;

&lt;h2 id=&quot;demo&quot;&gt;Demo&lt;/h2&gt;

&lt;p&gt;This section covers deployment of a local cluster with Istio service mesh, KubeVirt installation and creation of an Istio-enabled virtual machine.
Finally, Kiali dashboard is used to examine both inbound and outbound traffic of the created virtual machine.&lt;/p&gt;

&lt;h3 id=&quot;run-kubernetes-cluster&quot;&gt;Run Kubernetes cluster&lt;/h3&gt;

&lt;p&gt;In this blog post, we are going to use &lt;a href=&quot;https://github.com/kubevirt/kubevirtci&quot;&gt;kubevirtci&lt;/a&gt; as our Kubernetes ephemeral cluster provider.&lt;/p&gt;

&lt;p&gt;Follow these steps to deploy a local cluster with pre-installed Istio service mesh:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;git clone https://github.com/kubevirt/kubevirtci
&lt;span class=&quot;nb&quot;&gt;cd &lt;/span&gt;kubevirtci
&lt;span class=&quot;nb&quot;&gt;export &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;KUBEVIRTCI_TAG&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;2108222252-0007793
&lt;span class=&quot;c&quot;&gt;# Pin to version used in this blog post in case&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# k8s-1.21 provider version disappears in the future&lt;/span&gt;
git checkout &lt;span class=&quot;nv&quot;&gt;$KUBEVIRTCI_TAG&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;export &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;KUBEVIRT_NUM_NODES&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;2
&lt;span class=&quot;nb&quot;&gt;export &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;KUBEVIRT_PROVIDER&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;k8s-1.21
&lt;span class=&quot;nb&quot;&gt;export &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;KUBEVIRT_DEPLOY_ISTIO&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;true
export &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;KUBEVIRT_WITH_CNAO&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;true
&lt;/span&gt;make cluster-up
&lt;span class=&quot;nb&quot;&gt;export &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;KUBECONFIG&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;$(&lt;/span&gt;./cluster-up/kubeconfig.sh&lt;span class=&quot;si&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;For details about Istio configuration, see Istio kubevirtci &lt;a href=&quot;https://github.com/kubevirt/kubevirtci/blob/2108081530-91f55e3/cluster-provision/k8s/1.21/istio.sh&quot;&gt;install script&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;install-kubevirt&quot;&gt;Install Kubevirt&lt;/h3&gt;

&lt;p&gt;Following KubeVirt &lt;a href=&quot;https://kubevirt.io/user-guide/operations/installation/#installing-kubevirt-on-kubernetes&quot;&gt;user guide&lt;/a&gt; to install released version &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;v0.43.0&lt;/code&gt;:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;export &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;RELEASE&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;v0.43.0
kubectl apply &lt;span class=&quot;nt&quot;&gt;-f&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;https://github.com/kubevirt/kubevirt/releases/download/&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;RELEASE&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;/kubevirt-operator.yaml&quot;&lt;/span&gt;
kubectl apply &lt;span class=&quot;nt&quot;&gt;-f&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;https://github.com/kubevirt/kubevirt/releases/download/&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;RELEASE&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;/kubevirt-cr.yaml&quot;&lt;/span&gt;
kubectl &lt;span class=&quot;nt&quot;&gt;-n&lt;/span&gt; kubevirt &lt;span class=&quot;nb&quot;&gt;wait &lt;/span&gt;kv kubevirt &lt;span class=&quot;nt&quot;&gt;--timeout&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;180s &lt;span class=&quot;nt&quot;&gt;--for&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;condition&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;Available
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;install-istio-addons&quot;&gt;Install Istio addons&lt;/h3&gt;

&lt;p&gt;While the ephemeral kubevirtci installs core Istio components, addons like Kiali dashboard are not installed by default.
Download Istio manifests and client binary by running the following command:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;export &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;ISTIO_VERSION&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;1.10.0
curl &lt;span class=&quot;nt&quot;&gt;-L&lt;/span&gt; https://istio.io/downloadIstio | sh -
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;and export path to the istioctl binary by following the output of the above command.&lt;/p&gt;

&lt;p&gt;Finally, deploy Kiali, Jaeger and Prometheus addons:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl create &lt;span class=&quot;nt&quot;&gt;-f&lt;/span&gt; istio-&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;ISTIO_VERSION&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;/samples/addons/kiali.yaml
kubectl create &lt;span class=&quot;nt&quot;&gt;-f&lt;/span&gt; istio-&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;ISTIO_VERSION&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;/samples/addons/jaeger.yaml
kubectl create &lt;span class=&quot;nt&quot;&gt;-f&lt;/span&gt; istio-&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;ISTIO_VERSION&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;/samples/addons/prometheus.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; If there are errors when installing the addons, try running the command again. There may be timing issues which will be resolved when the command is run again.&lt;/p&gt;

&lt;h3 id=&quot;prepare-target-namespace&quot;&gt;Prepare target namespace&lt;/h3&gt;

&lt;p&gt;Before creating virtual machines, the target namespace needs to be configured for the Istio sidecar to be injected and working properly.
This involves adding a label and creating a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NetworkAttachmentDefinition&lt;/code&gt; in the target namespace.&lt;/p&gt;

&lt;h4 id=&quot;istio-sidecar-injection&quot;&gt;Istio sidecar injection&lt;/h4&gt;

&lt;p&gt;Istio supports &lt;a href=&quot;https://istio.io/latest/docs/setup/additional-setup/sidecar-injection/&quot;&gt;two ways of injecting&lt;/a&gt; a sidecar to a pod - automatic and manual. For simplicity, we will only consider automatic sidecar injection in this demo, which is enabled by adding &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;istio-injection=enabled&lt;/code&gt; label to target namespace:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl label namespace default istio-injection&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;enabled
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;network-attachment-definiton&quot;&gt;Network attachment definiton&lt;/h4&gt;

&lt;p&gt;When Multus is installed in k8s cluster, a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NetworkAttachmentDefinition&lt;/code&gt; called &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;istio-cni&lt;/code&gt; &lt;strong&gt;must&lt;/strong&gt; be created in &lt;strong&gt;each&lt;/strong&gt; namespace where Istio sidecar containers are to be used:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;cat&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;no&quot;&gt;EOF&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt; | kubectl create -f -
apiVersion: &quot;k8s.cni.cncf.io/v1&quot;
kind: NetworkAttachmentDefinition
metadata:
  name: istio-cni
&lt;/span&gt;&lt;span class=&quot;no&quot;&gt;EOF
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NetworkAttachmentDefinition&lt;/code&gt; spec is empty, as its only purpose is to trigger the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;istio-cni&lt;/code&gt; binary, which configures the in-pod traffic routing.&lt;/p&gt;

&lt;h3 id=&quot;topology&quot;&gt;Topology&lt;/h3&gt;

&lt;p&gt;To demonstrate monitoring and tracing capabilities, we will create two VMIs within Istio service mesh:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;istio-vmi&lt;/code&gt; repeatedly requests external HTTP service kubevirt.io, and serves a simple HTTP server on port 8080,&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cirros-vmi&lt;/code&gt; repeatedly request the HTTP service running on the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;istio-vmi&lt;/code&gt; VMI.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;With this setup, both inbound and outbound
traffic metrics can be observed in Kiali dashboard for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;istio-vmi&lt;/code&gt;.&lt;/p&gt;

&lt;h3 id=&quot;create-vmi-resources&quot;&gt;Create VMI resources&lt;/h3&gt;

&lt;p&gt;An Istio aware virtual machine &lt;strong&gt;must&lt;/strong&gt; be annotated with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sidecar.istio.io/inject: &quot;true&quot;&lt;/code&gt;, regardless of used Istio injection mechanism.
Without this annotation, traffic would not be properly routed through the istio proxy sidecar.
Additonally, Istio uses &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;app&lt;/code&gt; label for adding contextual information to the collected telemetry.
Both, the annotation and label can be seen in the following virtual machine example:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;cat&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;no&quot;&gt;EOF&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt; | kubectl create -f -
apiVersion: kubevirt.io/v1
kind: VirtualMachineInstance
metadata:
  annotations:
    sidecar.istio.io/inject: &quot;true&quot;
  labels:
    app: istio-vmi
  name: istio-vmi
spec:
  domain:
    devices:
      interfaces:
        - name: default
          masquerade: {}
          ports:
            - port: 8080
      disks:
        - disk:
            bus: virtio
          name: containerdisk
        - disk:
            bus: virtio
          name: cloudinitdisk
    resources:
      requests:
        memory: 1024M
  networks:
    - name: default
      pod: {}
  readinessProbe:
    httpGet:
      port: 8080
    initialDelaySeconds: 120
    periodSeconds: 10
    timeoutSeconds: 5
    failureThreshold: 3
    successThreshold: 3
  terminationGracePeriodSeconds: 0
  volumes:
    - name: containerdisk
      containerDisk:
        image: kubevirt/fedora-cloud-container-disk-demo:devel
    - cloudInitNoCloud:
        userData: |
          #cloud-config
          password: fedora
          chpasswd: { expire: False }
          runcmd:
          - dnf install -y screen nc
          - while true ; do sh -c &quot;nc -lp 8080 -c  &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\&quot;&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;echo -e 'HTTP/1.1 200 OK&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;Hello'&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\&quot;&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt; &quot; ; done &amp;amp;
          - while true ; do curl kubevirt.io &amp;gt;out 2&amp;gt;/dev/null ; sleep 1 ; done &amp;amp;
      name: cloudinitdisk
&lt;/span&gt;&lt;span class=&quot;no&quot;&gt;EOF
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The cloud init section of the VMI runs two loops requesting &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kubevirt.io&lt;/code&gt; website every second to generate outbound traffic (from the VMI) and serving simple HTTP server on port &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;8080&lt;/code&gt;, which will be used for monitoring of inbound traffic (to the VMI).&lt;/p&gt;

&lt;p&gt;Let‚Äôs also create a service for the VMI that will be used to access the http server in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;istio-vmi&lt;/code&gt;:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;cat&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;no&quot;&gt;EOF&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt; | kubectl create -f-
apiVersion: v1
kind: Service
metadata:
  name: istio-vmi-svc
spec:
  selector:
    app: istio-vmi
  ports:
  - port: 8080
    protocol: TCP
&lt;/span&gt;&lt;span class=&quot;no&quot;&gt;EOF
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Finally, create the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cirros-vmi&lt;/code&gt; VMI, for the purpose of generating inbound traffic to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;istio-vmi&lt;/code&gt; VMI:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;cat&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;no&quot;&gt;EOF&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt; | kubectl create -f -
apiVersion: kubevirt.io/v1
kind: VirtualMachineInstance
metadata:
  annotations:
    sidecar.istio.io/inject: &quot;true&quot;
  name: cirros-vmi
  labels:
    app: cirros-vmi
spec:
  domain:
    devices:
      interfaces:
        - name: default
          masquerade: {}
      disks:
        - disk:
            bus: virtio
          name: containerdisk
        - disk:
            bus: virtio
          name: cloudinitdisk
    resources:
      requests:
        memory: 128M
  networks:
    - name: default
      pod: {}
  terminationGracePeriodSeconds: 0
  volumes:
    - name: containerdisk
      containerDisk:
        image: kubevirt/cirros-container-disk-demo:devel
    - name: cloudinitdisk
      cloudInitNoCloud:
        userData: |
          #!/bin/sh
          while true ; do curl istio-vmi-svc.default.svc.cluster.local:8080 ; sleep 1 ; done
&lt;/span&gt;&lt;span class=&quot;no&quot;&gt;EOF
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Wait for the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;istio-vmi&lt;/code&gt; to be ready:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl &lt;span class=&quot;nb&quot;&gt;wait&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--for&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;condition&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;ready &lt;span class=&quot;nt&quot;&gt;--timeout&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;180s pod &lt;span class=&quot;nt&quot;&gt;-l&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;app&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;istio-vmi
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;After creating the VMIs, the corresponding virt-launcher pods should have 3 ready containers, as shown in the snippet below:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl get pods
NAME                             READY   STATUS    RESTARTS   AGE
virt-launcher-istio-vmi-XYZ      3/3     Running   0          4m13s
virt-launcher-cirros-vmi-XYZ     3/3     Running   0          2m21s
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Istioctl proxy-status&lt;/code&gt; should report that the sidecar proxies running inside the virt-launcher pods have synced with Istio control plane:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;istioctl proxy-status
NAME                                          CDS        LDS        EDS        RDS          ISTIOD                      VERSION
virt-launcher-cirros-vmi-9f765.default        SYNCED     SYNCED     SYNCED     SYNCED       istiod-7d96484d6b-5d79g     1.10.0
virt-launcher-istio-vmi-99t8t.default         SYNCED     SYNCED     SYNCED     SYNCED       istiod-7d96484d6b-nk4cd     1.10.0
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; Displaying only relevant VMI entities.&lt;/p&gt;

&lt;h3 id=&quot;monitor-traffic-in-kiali-dashboard&quot;&gt;Monitor traffic in Kiali dashboard&lt;/h3&gt;

&lt;p&gt;With both VMIs up and running, we can open the Kiali dashboard and observe the traffic metrics.
Run the following command, to access Kiali dashboard:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;istioctl dashboard kiali
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;topology-graph&quot;&gt;Topology graph&lt;/h4&gt;

&lt;p&gt;Let‚Äôs start by navigating to the topology graph by clicking the Graph menu item.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;div class=&quot;my-gallery&quot; itemscope=&quot;&quot; itemtype=&quot;http://schema.org/ImageGallery&quot;&gt;
  &lt;figure itemprop=&quot;associatedMedia&quot; itemscope=&quot;&quot; itemtype=&quot;http://schema.org/ImageObject&quot;&gt;
    &lt;a href=&quot;/assets/2021-08-13-Virtual-machines-in-Istio-service-mesh/topology-graph.png&quot; itemprop=&quot;contentUrl&quot; data-size=&quot;800x530&quot;&gt;
      &lt;img src=&quot;/assets/2021-08-13-Virtual-machines-in-Istio-service-mesh/topology-graph.png&quot; itemprop=&quot;thumbnail&quot; width=&quot;100%&quot; alt=&quot;Topology graph&quot; /&gt;
    &lt;/a&gt;
    &lt;figcaption itemprop=&quot;Topology graph of deployed VMIs showing the traffic flows&quot;&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;In the topology graph, we can observe the following traffic flows:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;requests from &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cirros-vmi&lt;/code&gt; to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;istio-vmi&lt;/code&gt; via &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;istio-vmi-svc&lt;/code&gt; service,&lt;/li&gt;
  &lt;li&gt;requests from &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;istio-vmi&lt;/code&gt; to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PasstroughCluster&lt;/code&gt;. The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PastroughCluster&lt;/code&gt; marks  destinations external to our service mesh.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;workloads&quot;&gt;Workloads&lt;/h4&gt;

&lt;p&gt;Navigate to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;istio-vmi&lt;/code&gt; workload overview by clicking the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Workloads&lt;/code&gt; menu item and selecting &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;istio-vmi&lt;/code&gt; from the list.&lt;/p&gt;

&lt;p&gt;The overview page presents partial topology graph with traffic related to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;istio-vmi&lt;/code&gt;. In our case, this graph is the same as the graph of our entire mesh.&lt;/p&gt;

&lt;p&gt;Navigate to Inbound Metrics tab to see metrics charts of inbound traffic.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;div class=&quot;my-gallery&quot; itemscope=&quot;&quot; itemtype=&quot;http://schema.org/ImageGallery&quot;&gt;
  &lt;figure itemprop=&quot;associatedMedia&quot; itemscope=&quot;&quot; itemtype=&quot;http://schema.org/ImageObject&quot;&gt;
    &lt;a href=&quot;/assets/2021-08-13-Virtual-machines-in-Istio-service-mesh/istio-blog-vmi-istio-inbound-traffic.png&quot; itemprop=&quot;contentUrl&quot; data-size=&quot;800x530&quot;&gt;
      &lt;img src=&quot;/assets/2021-08-13-Virtual-machines-in-Istio-service-mesh/istio-blog-vmi-istio-inbound-traffic.png&quot; itemprop=&quot;thumbnail&quot; width=&quot;100%&quot; alt=&quot;Inbound traffic metrics&quot; /&gt;
    &lt;/a&gt;
    &lt;figcaption itemprop=&quot;Inbound traffic metrics for `istio-vmi`&quot;&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;In Request volume chart we can see that number of requests stabilizes at around &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;1 ops&lt;/code&gt;, which matches our loop sending one reqest per second. Request throughput chart reveals that the requests consume around &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;4 kbit/s&lt;/code&gt; of bandwidth.
Remaining two charts provide information about Request duration and size.&lt;/p&gt;

&lt;p&gt;The same metrics are collected for outbound traffic as well, which can be seen in Outbound Metrics tab.&lt;/p&gt;

&lt;h2 id=&quot;cluster-teardown&quot;&gt;Cluster teardown&lt;/h2&gt;

&lt;p&gt;Run the following command to deprovision the ephemeral cluster:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;make cluster-down
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;KubeVirt introduced &lt;a href=&quot;https://kubevirt.io/user-guide/virtual_machines/istio_service_mesh/&quot;&gt;support for Istio&lt;/a&gt;, allowing virtual machines to be part of a service mesh.&lt;/p&gt;

&lt;p&gt;This blog post covered running KubeVirt virtual machine in Istio service mesh using an ephemeral kubevirtci cluster. Kiali dashboard was used to observe inbound and outbound traffic of a virtual machine.&lt;/p&gt;</content><author><name>Radim Hrazdil</name></author><category term="news" /><category term="kubevirt" /><category term="istio" /><category term="virtual machine" /><category term="VM" /><category term="service mesh" /><category term="mesh" /><summary type="html">Introduction</summary></entry><entry><title type="html">KubeVirt v0.44.0</title><link href="https://kubevirt.io//2021/changelog-v0.44.0.html" rel="alternate" type="text/html" title="KubeVirt v0.44.0" /><published>2021-08-09T00:00:00+00:00</published><updated>2021-08-09T00:00:00+00:00</updated><id>https://kubevirt.io//2021/changelog-v0.44.0</id><content type="html" xml:base="https://kubevirt.io//2021/changelog-v0.44.0.html">&lt;h2 id=&quot;v0440&quot;&gt;v0.44.0&lt;/h2&gt;

&lt;p&gt;Released on: Mon Aug 9 14:20:14 2021 +0000&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;[PR #6058][acardace] Fix virt-launcher exit pod race condition&lt;/li&gt;
  &lt;li&gt;[PR #6035][davidvossel] Addition of perfscale-audit tool for auditing performance of control plane during stress tests&lt;/li&gt;
  &lt;li&gt;[PR #6145][acardace] virt-launcher: disable unencrypted TCP socket for libvirtd.&lt;/li&gt;
  &lt;li&gt;[PR #6163][davidvossel] Handle qemu processes in defunc (zombie) state&lt;/li&gt;
  &lt;li&gt;[PR #6105][ashleyschuett] Add VirtualMachineInstancesPerNode to KubeVirt CR under Spec.Configuration&lt;/li&gt;
  &lt;li&gt;[PR #6104][zcahana] Report FailedUnschedulable VM status when scheduling errors occur&lt;/li&gt;
  &lt;li&gt;[PR #5905][davidvossel] VM CrashLoop detection and Exponential Backoff&lt;/li&gt;
  &lt;li&gt;[PR #6070][acardace] Initiate Live-Migration using a unix socket (exposed by virt-handler) instead of an additional TCP&amp;lt;-&amp;gt;Unix migration proxy started by virt-launcher&lt;/li&gt;
  &lt;li&gt;[PR #5728][vasiliy-ul] Live migration of VMs with hotplug volumes is now enabled&lt;/li&gt;
  &lt;li&gt;[PR #6109][rmohr] Fix virt-controller SCC: Reflect the need for NET_BIND_SERVICE in the virt-controller SCC.&lt;/li&gt;
  &lt;li&gt;[PR #5942][ShellyKa13] Integrate guest agent to online VM snapshot&lt;/li&gt;
  &lt;li&gt;[PR #6034][ashleyschuett] Go version updated to version 1.16.6&lt;/li&gt;
  &lt;li&gt;[PR #6040][yuhaohaoyu] Improved debuggability by keeping the environment of a failed VMI alive.&lt;/li&gt;
  &lt;li&gt;[PR #6068][dhiller] Add check that not all tests have been skipped&lt;/li&gt;
  &lt;li&gt;[PR #6041][xpivarc] [Experimental] Virt-launcher can run as non-root user&lt;/li&gt;
  &lt;li&gt;[PR #6062][iholder-redhat] replace dead ‚Äústress‚Äù binary with new, maintained, ‚Äústress-ng‚Äù binary&lt;/li&gt;
  &lt;li&gt;[PR #6029][mhenriks] CDI to 1.36.0 with DataSource support&lt;/li&gt;
  &lt;li&gt;[PR #4089][victortoso] Add support to USB Redirection with usbredir&lt;/li&gt;
  &lt;li&gt;[PR #5946][vatsalparekh] Add guest-agent based ping probe&lt;/li&gt;
  &lt;li&gt;[PR #6005][acardace] make containerDisk validation memory usage limit configurable&lt;/li&gt;
  &lt;li&gt;[PR #5791][zcahana] Added a READY column to the tabular output of ‚Äúkubectl get vm/vmi‚Äù&lt;/li&gt;
  &lt;li&gt;[PR #6006][awels] DataVolumes created by DataVolumeTemplates will follow the associated VMs priority class.&lt;/li&gt;
  &lt;li&gt;[PR #5982][davidvossel] Reduce vmi Update collisions (http code 409) during startup&lt;/li&gt;
  &lt;li&gt;[PR #5891][akalenyu] BugFix: Pending VMIs when creating concurrent bulk of VMs backed by WFFC DVs&lt;/li&gt;
  &lt;li&gt;[PR #5925][rhrazdil] Fix issue with Windows VMs not being assigned IP address configured in network-attachment-definition IPAM.&lt;/li&gt;
  &lt;li&gt;[PR #6007][rmohr] Fix: The bandwidth limitation on migrations is no longer ignored. Caution: The default bandwidth limitation of 64Mi is changed to ‚Äúunlimited‚Äù to not break existing installations.&lt;/li&gt;
  &lt;li&gt;[PR #4944][kwiesmueller] Add &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/portforward&lt;/code&gt; subresource to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;VirtualMachine&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;VirtualMachineInstance&lt;/code&gt; that can tunnel TCP traffic through the API Server using a websocket stream.&lt;/li&gt;
  &lt;li&gt;[PR #5402][alicefr] Integration of libguestfs-tools and added new command &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;guestfs&lt;/code&gt; to virtctl&lt;/li&gt;
  &lt;li&gt;[PR #5953][ashleyschuett] Allow Failed VMs to be stopped when using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--force --gracePeriod 0&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;[PR #5876][mlsorensen] KubeVirt CR supports specifying a runtime class for virt-launcher pods via ‚ÄòlauncherRuntimeClass‚Äô.&lt;/li&gt;
&lt;/ul&gt;</content><author><name>kubeü§ñ</name></author><category term="releases" /><category term="release notes" /><category term="changelog" /><summary type="html">v0.44.0</summary></entry><entry><title type="html">Kubernetes Authentication Options using KubeVirt Client Library</title><link href="https://kubevirt.io//2021/kubevirt-api-auth.html" rel="alternate" type="text/html" title="Kubernetes Authentication Options using KubeVirt Client Library" /><published>2021-07-16T00:00:00+00:00</published><updated>2021-07-16T00:00:00+00:00</updated><id>https://kubevirt.io//2021/kubevirt-api-auth</id><content type="html" xml:base="https://kubevirt.io//2021/kubevirt-api-auth.html">&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#introduction&quot;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#requirements&quot;&gt;Requirements&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#setup&quot;&gt;Setup&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#compiling-our-test-application&quot;&gt;Compiling our test application&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#running-our-application-externally-leveraging-a-kubeconfig-file&quot;&gt;Running our application externally leveraging a kubeconfig file&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#using-the-default-kubeconfig&quot;&gt;Using the default kubeconfig&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#creating-a-kubeconfig-for-the-service-account&quot;&gt;Creating a kubeconfig for the service account&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#running-in-a-kubernetes-cluster&quot;&gt;Running in a Kubernetes Cluster&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#extending-rbac-role-across-namespaces&quot;&gt;Extending RBAC Role across Namespaces&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#creating-custom-rbac-roles&quot;&gt;Creating Custom RBAC Roles&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#conclusion&quot;&gt;Conclusion&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#references&quot;&gt;References&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;Most interaction with the KubeVirt service can be handled using the &lt;em&gt;virtctl&lt;/em&gt; command, or raw yaml applied to your Kubernetes cluster. But what if you want to have more direct programmatic control over the instantiation and management of those virtual machines? The KubeVirt project supplies a Go client library for interacting with KubeVirt called &lt;a href=&quot;https://github.com/kubevirt/client-go&quot;&gt;client-go&lt;/a&gt;. This library allows you to write your own applications that interact directly with the KubeVirt api quickly and easily.&lt;/p&gt;

&lt;p&gt;In this post, we will use a simple application to demonstrate how the KubeVirt client library authenticates with your Kubernetes cluster both in and out of your cluster. This application is based on the example application in the ‚Äúclient-go‚Äù library with a few small modifications to it, to allow for running both locally and within in the cluster. This tutorial assumes you have some knowledge of Go, and is not meant to be a Go training doc.&lt;/p&gt;

&lt;h2 id=&quot;requirements&quot;&gt;Requirements&lt;/h2&gt;

&lt;p&gt;In order to compile and run the test application locally you will need to have the Go programming language installed on your machine. If you do not have the latest version of Go installed, follow the steps on the &lt;a href=&quot;https://golang.org/dl/&quot;&gt;Downloads&lt;/a&gt; page of the Go  web site before proceeding with the rest of the steps in this blog. The steps listed here were tested with Go version 1.16.&lt;/p&gt;

&lt;p&gt;You will need a Kubernetes cluster running with the KubeVirt operator installed. If you do not have a cluster available, the easiest way to do this is to follow the steps outlined in the &lt;a href=&quot;https://kubevirt.io/quickstart_minikube/&quot;&gt;Quick Start with Minikube&lt;/a&gt; lab.&lt;/p&gt;

&lt;p&gt;The example application we will be using to demonstrate the authentication methods lists out the VMI and VM instances in your cluster in the current namespace. If you do not have any running VMs in your cluster, be sure to create at least one new virtual machine instance in your cluster. For guidance in creating a quick test vm see the &lt;a href=&quot;https://kubevirt.io/labs/kubernetes/lab1.html&quot;&gt;Use KubeVirt&lt;/a&gt; lab.&lt;/p&gt;

&lt;h2 id=&quot;setup&quot;&gt;Setup&lt;/h2&gt;

&lt;h3 id=&quot;compiling-our-test-application&quot;&gt;Compiling our test application&lt;/h3&gt;

&lt;p&gt;Start by cloning the example application repo &lt;a href=&quot;https://github.com/xphyr/kubevirt-apiauth&quot;&gt;https://github.com/xphyr/kubevirt-apiauth&lt;/a&gt; and compiling our test application:&lt;/p&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;git clone https://github.com/xphyr/kubevirt-apiauth.git
&lt;span class=&quot;nb&quot;&gt;cd &lt;/span&gt;kubevirt-apiauth/listvms
go build
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Once the program compiles, test to ensure that the application compiled correctly. If you have a working Kubernetes context, running this command may return some values. If you do not have a current context, you will get an error. This is OK, we will discuss authentication next.&lt;/p&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;./listvms
2021/06/23 16:51:28 cannot obtain KubeVirt vm list: Get &lt;span class=&quot;s2&quot;&gt;&quot;http://localhost:8080/apis/kubevirt.io/v1alpha3/namespaces/default/virtualmachines&quot;&lt;/span&gt;: dial tcp 127.0.0.1:8080: connect: connection refused
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;As long as the program runs, you are all set to move onto the next step.&lt;/p&gt;

&lt;h2 id=&quot;running-our-application-externally-leveraging-a-kubeconfig-file&quot;&gt;Running our application externally leveraging a kubeconfig file&lt;/h2&gt;

&lt;p&gt;The default authentication file for Kubernetes is the &lt;a href=&quot;https://kubernetes.io/docs/concepts/configuration/organize-cluster-access-kubeconfig/&quot;&gt;kubeconfig&lt;/a&gt; file. We will not be going into details of this file, but you can click the link to goto the documentation on the kubeconfig file to learn more about it. All you need to know at this time is that when you use the &lt;em&gt;kubectl&lt;/em&gt; command you are using a kubeconfig file for your authentication.&lt;/p&gt;

&lt;h3 id=&quot;using-the-default-kubeconfig&quot;&gt;Using the default kubeconfig&lt;/h3&gt;

&lt;p&gt;If you haven‚Äôt already done so, validate that you have a successful connection to your cluster with the ‚Äú&lt;em&gt;kubectl&lt;/em&gt;‚Äù command:&lt;/p&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl get nodes
NAME       STATUS   ROLES                  AGE     VERSION
minikube   Ready    control-plane,master   5d21h   v1.20.7
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We now have a valid kubeconfig. On *nix OS such as Linux and OSX, this file is stored in your home directory at &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;~/.kube/config&lt;/code&gt;. You should now be able to run our test application and get some results (assuming you have some running vms in your cluster).&lt;/p&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;./listvms/listvms
Type                       Name       Namespace     Status
VirtualMachine             testvm     default       &lt;span class=&quot;nb&quot;&gt;false
&lt;/span&gt;VirtualMachineInstance     testvm     default       Scheduled
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This is great, but there is an issue. The authentication method we used is your primary Kubernetes authentication. It has roles and permissions to do many different things in your k8s cluster. Wouldn‚Äôt it be better if we could scope that authentication and ensure that your application had a dedicated account, with only the proper permissions to interact with just what your application will need. This is what Kubernetes &lt;strong&gt;Service Accounts&lt;/strong&gt; are for.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://kubernetes.io/docs/reference/access-authn-authz/service-accounts-admin/&quot;&gt;Service Accounts&lt;/a&gt; are accounts for processes as opposed to users. By default they are scoped to a namespace, but you can give service accounts access to other namespaces through RBAC rules that we will discuss later. In this demo, we will be using the ‚Äú&lt;em&gt;default&lt;/em&gt;‚Äù project/namespace, so the service account we create will be initially scoped only to this namespace.&lt;/p&gt;

&lt;p&gt;Start by creating a new service account called ‚Äúmykubevirtrunner‚Äù using your default Kubernetes account:&lt;/p&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl create sa mykubevirtrunner
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl describe sa mykubevirtrunner
Name:                mykubevirtrunner
Namespace:           default
Labels:              &amp;lt;none&amp;gt;
Annotations:         &amp;lt;none&amp;gt;
Image pull secrets:  &amp;lt;none&amp;gt;
Mountable secrets:   mykubevirtrunner-token-pd2mq
Tokens:              mykubevirtrunner-token-pd2mq
Events:              &amp;lt;none&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In the describe output you can see that a token and a mountable secret have been created. Let‚Äôs take a look at the contents of the secret:&lt;/p&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl describe secret mykubevirtrunner-token-pd2mq
Name:         mykubevirtrunner-token-pd2mq
Namespace:    default
Labels:       &amp;lt;none&amp;gt;
Annotations:  kubernetes.io/service-account.name: mykubevirtrunner
              kubernetes.io/service-account.uid: f401493b-658a-489d-bcce-0ccce39160a0

Type:  kubernetes.io/service-account-token

Data
&lt;span class=&quot;o&quot;&gt;====&lt;/span&gt;
namespace:  7 bytes
token:      eyJhbGciOiJS...
ca.crt:     1111 bytes

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The data listed for the ‚Äútoken‚Äù key is the information we will use in the next step, your output will be much longer, it has been truncated for this document. Ensure when copying the value that you get the entire token value.&lt;/p&gt;

&lt;h3 id=&quot;creating-a-kubeconfig-for-the-service-account&quot;&gt;Creating a kubeconfig for the service account&lt;/h3&gt;

&lt;p&gt;We will create a new kubeconfig file that leverages the service account and token we just created. The easiest way to do this is to create an empty kubeconfig file, and use the ‚Äú&lt;em&gt;kubectl&lt;/em&gt;‚Äù command to log in with the new token. Open a NEW terminal window. This will be the window we use for the service account. In this new terminal window start by setting the KUBECONFIG environment variable to point to a file in our local directory, and then using the ‚Äú&lt;em&gt;kubectl&lt;/em&gt;‚Äù command to generate a new kubeconfig file:&lt;/p&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;export &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;KUBECONFIG&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;$(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;pwd&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;)&lt;/span&gt;/sa-kubeconfig
kubectl config set-cluster minikube &lt;span class=&quot;nt&quot;&gt;--server&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;https://&amp;lt;update IP address&amp;gt;:8443 &lt;span class=&quot;nt&quot;&gt;--insecure-skip-tls-verify&lt;/span&gt;
kubectl config set-credentials mykubevirtrunner &lt;span class=&quot;nt&quot;&gt;--token&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&amp;lt;&lt;span class=&quot;nb&quot;&gt;paste &lt;/span&gt;token from last step here&amp;gt;
kubectl config set-context minikube &lt;span class=&quot;nt&quot;&gt;--cluster&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;minikube &lt;span class=&quot;nt&quot;&gt;--namespace&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;default &lt;span class=&quot;nt&quot;&gt;--user&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;mykubevirtrunner
kubectl config use-context minikube
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We can test that the new kubeconfig file is working by running a kubectl command:&lt;/p&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl get pods
Error from server &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;Forbidden&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;: pods is forbidden: User &lt;span class=&quot;s2&quot;&gt;&quot;system:serviceaccount:default:mykubevirtrunner&quot;&lt;/span&gt; cannot list resource &lt;span class=&quot;s2&quot;&gt;&quot;pods&quot;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;in &lt;/span&gt;API group &lt;span class=&quot;s2&quot;&gt;&quot;&quot;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;in &lt;/span&gt;the namespace &lt;span class=&quot;s2&quot;&gt;&quot;default&quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Note that the ‚ÄúUser‚Äù is now listed as ‚Äú&lt;em&gt;system:serviceaccount:default:mykubevirtrunner&lt;/em&gt;‚Äù so we know we are using our new service account. Now try running our test program and note that it is using the service account as well:&lt;/p&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;listvms/listvms
2021/07/07 14:53:23 cannot obtain KubeVirt vm list: virtualmachines.kubevirt.io is forbidden: User &lt;span class=&quot;s2&quot;&gt;&quot;system:serviceaccount:default:mykubevirtrunner&quot;&lt;/span&gt; cannot list resource &lt;span class=&quot;s2&quot;&gt;&quot;virtualmachines&quot;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;in &lt;/span&gt;API group &lt;span class=&quot;s2&quot;&gt;&quot;kubevirt.io&quot;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;in &lt;/span&gt;the namespace &lt;span class=&quot;s2&quot;&gt;&quot;default&quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;You can see we are now using our service account in our application, but that service account doesn‚Äôt have the right permissions‚Ä¶ We now need to assign a role to our service account to give it the proper API access. We will start simple and give the service account the &lt;strong&gt;kubevirt.io:view&lt;/strong&gt; role, which will allow the service account to see the KubeVirt objects within the ‚Äú&lt;em&gt;default&lt;/em&gt;‚Äù namespace:&lt;/p&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl create clusterrolebinding kubevirt-viewer &lt;span class=&quot;nt&quot;&gt;--clusterrole&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;kubevirt.io:view &lt;span class=&quot;nt&quot;&gt;--serviceaccount&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;default:mykubevirtrunner
clusterrolebinding.rbac.authorization.k8s.io/kubevirt-viewer created
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Now run the &lt;em&gt;listvms&lt;/em&gt; command again:&lt;/p&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;./listvms/listvms
Type                       Name                    Namespace     Status
VirtualMachineInstance     vm-fedora-ephemeral     myvms         Running
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Success! Our application is now using the service account that we created for authentication to the cluster. The service account can be extended by adding additional default roles to the account, or by creating custom roles that limit the scope of the service account to only the exact actions you want to take. When you install KubeVirt you get a set of default roles including ‚ÄúView‚Äù, ‚ÄúEdit‚Äù and ‚ÄúAdmin‚Äù. Additional details about these roles are available here: &lt;a href=&quot;https://kubevirt.io/user-guide/operations/authorization/&quot;&gt;KubeVirt Default RBAC Cluster Roles&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;running-in-a-kubernetes-cluster&quot;&gt;Running in a Kubernetes Cluster&lt;/h2&gt;

&lt;p&gt;So all of this is great if you want to run the application outside of your cluster ‚Ä¶ but what if you want your application to run INSIDE you cluster. You could create a kubeconfig file, and add it to your namespace as a secret and then mount that secret as a volume inside your pod, but there is an easier way that continues to leverage the service account that we created. By default Kubernetes creates a few environment variables for every pod that indicate that the container is running within Kubernetes, and it makes a Kubernetes authentication token for the service account that the container is running as available at /var/run/secrets/kubernetes.io/serviceaccount/token. The client-go KubeVirt library can detect that it is running inside a Kubernetes hosted container and will transparently use the authentication token provided with no additional configuration needed.&lt;/p&gt;

&lt;p&gt;A container image with the listvms binary is available at &lt;strong&gt;quay.io/markd/listvms&lt;/strong&gt;. We can start a copy of this container using the deployment yaml file located in the ‚Äòlistvms/listvms_deployment.yaml‚Äô file.&lt;/p&gt;

&lt;p&gt;Switch back to your original terminal window that is using your primary kubeconfig file, and using the ‚Äú&lt;em&gt;kubectl&lt;/em&gt;‚Äù command deploy one instance of the test pod, and then check the logs of the pod:&lt;/p&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl create &lt;span class=&quot;nt&quot;&gt;-f&lt;/span&gt; listvms/listvms_deployment.yaml
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl get pods
NAME                                      READY   STATUS    RESTARTS   AGE
listvms-7b8f865c8d-2zqqn                  1/1     Running   0          7m30s
virt-launcher-vm-fedora-ephemeral-4ljg4   2/2     Running   0          24h
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl logs listvms-7b8f865c8d-2zqqn
2021/07/07 19:06:42 cannot obtain KubeVirt vm list: virtualmachines.kubevirt.io is forbidden: User &lt;span class=&quot;s2&quot;&gt;&quot;system:serviceaccount:default:default&quot;&lt;/span&gt; cannot list resource &lt;span class=&quot;s2&quot;&gt;&quot;virtualmachines&quot;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;in &lt;/span&gt;API group &lt;span class=&quot;s2&quot;&gt;&quot;kubevirt.io&quot;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;in &lt;/span&gt;the namespace &lt;span class=&quot;s2&quot;&gt;&quot;default&quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;NOTE:&lt;/strong&gt; Be sure to deploy this demo application in a namespace that contains at least one running VM or VMI.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The application is unable to run the operation, because it is running as the default service account in the ‚Äú&lt;em&gt;default&lt;/em&gt;‚Äù namespace. If you remember previously we created a service account in this namespace called ‚Äúmykubevirtrunner‚Äù. We need only update the deployment to use this service account and we should see some success. Use the ‚Äúkubectl edit deployment/listvms‚Äù command to update the container spec to include the ‚ÄúserviceAccount: mykubevirtrunner‚Äù line as show below:&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    &lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;containers&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;listvms&lt;/span&gt;
          &lt;span class=&quot;na&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;quay.io/markd/listvms&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;serviceAccount&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;mykubevirtrunner&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;securityContext&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;pi&quot;&gt;{}&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;schedulerName&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;default-scheduler&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This change will trigger Kubernetes to redeploy your pod, using the new serviceAccount. We should now see some output from our program:&lt;/p&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl get pods
NAME                                      READY   STATUS    RESTARTS   AGE
listvms-7b8f865c8d-2qzzn                  1/1     Running   0          7m30s
virt-launcher-vm-fedora-ephemeral-4ljg4   2/2     Running   0          24h
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl logs listvms-7b8f865c8d-2qzzn
Type                       Name                    Namespace     Status
VirtualMachineInstance     vm-fedora-ephemeral     myvms         Running
awaiting signal
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;extending-rbac-role-across-namespaces&quot;&gt;Extending RBAC Role across Namespaces&lt;/h2&gt;

&lt;p&gt;As currently configured, the mykubevirtrunner service account can only ‚Äúview‚Äù KubeVirt resources within its own namespace. If we want to extend that ability to other namespaces, we can add the view role for other namespaces to the mykubevirtrunner serviceAccount.&lt;/p&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl create namespace myvms
&amp;lt;launch an addition vm here&amp;gt;
kubectl create clusterrolebinding kubevirt-viewer &lt;span class=&quot;nt&quot;&gt;--clusterrole&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;kubevirt.io:view &lt;span class=&quot;nt&quot;&gt;--serviceaccount&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;default:mykubevirtrunner &lt;span class=&quot;nt&quot;&gt;-n&lt;/span&gt; myvms
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We can test that the ServiceAccount has been updated to also have permissions to view in the ‚Äúmyvms‚Äù namespace by running our listvms command one more time, this time passing in the optional flag &lt;em&gt;‚Äìnamespaces&lt;/em&gt;. Switch to your terminal window that is using the service account kubeconfig file and run the following command:&lt;/p&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;listvms/listvms &lt;span class=&quot;nt&quot;&gt;--namespaces&lt;/span&gt; myvms
additional namespaces to check are:  myvms
Checking the following namespaces:  &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;default myvms]
Type                       Name       Namespace     Status
VirtualMachine             testvm     default       &lt;span class=&quot;nb&quot;&gt;false
&lt;/span&gt;VirtualMachineInstance     testvm     default       Scheduled
VirtualMachine             testvm     myvms         &lt;span class=&quot;nb&quot;&gt;false&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;You can see that now, the ServiceAccount can view the vm and vmi that are in both the default namespace as well as the &lt;em&gt;myvms&lt;/em&gt; namespace.&lt;/p&gt;

&lt;h2 id=&quot;creating-custom-rbac-roles&quot;&gt;Creating Custom RBAC Roles&lt;/h2&gt;

&lt;p&gt;In this demo we used RBAC roles created as part of the KubeVirt install. You can also create custom RBAC roles for KubeVirt. Documentation on how this can be done is available in the KubeVirt documentation &lt;a href=&quot;https://kubevirt.io/user-guide/operations/authorization/#creating-custom-rbac-roles&quot;&gt;Creating Custom RBAC Roles&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;It is possible to control and manage your KubeVirt machines with the use of Kubernetes service accounts and the ‚Äúclient-go‚Äù library. When using service accounts, you want to ensure that the account has the minimum role or permissions to do it‚Äôs job to ensure the security of your cluster. The ‚Äúclient-go‚Äù library gives you options on how you authenticate with your Kubernetes cluster, allowing you to deploy your application both in and out of your Kubernetes cluster.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/kubevirt/client-go&quot;&gt;KubeVirt Client Go&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://kubevirt.io/2018/KubeVirt-API-Access-Control.html&quot;&gt;KubeVirt API Access Control&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://kubevirt.io/user-guide/operations/authorization/&quot;&gt;KubeVirt Default RBAC Cluster Roles&lt;/a&gt;&lt;/p&gt;</content><author><name>Mark DeNeve</name></author><category term="news" /><category term="kubevirt" /><category term="go" /><category term="api" /><category term="authentication" /><summary type="html">Introduction Requirements Setup Compiling our test application Running our application externally leveraging a kubeconfig file Using the default kubeconfig Creating a kubeconfig for the service account Running in a Kubernetes Cluster Extending RBAC Role across Namespaces Creating Custom RBAC Roles Conclusion References</summary></entry><entry><title type="html">KubeVirt v0.43.0</title><link href="https://kubevirt.io//2021/changelog-v0.43.0.html" rel="alternate" type="text/html" title="KubeVirt v0.43.0" /><published>2021-07-09T00:00:00+00:00</published><updated>2021-07-09T00:00:00+00:00</updated><id>https://kubevirt.io//2021/changelog-v0.43.0</id><content type="html" xml:base="https://kubevirt.io//2021/changelog-v0.43.0.html">&lt;h2 id=&quot;v0430&quot;&gt;v0.43.0&lt;/h2&gt;

&lt;p&gt;Released on: Fri Jul 9 15:46:22 2021 +0000&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;[PR #5952][mhenriks] Use CDI beta API. CDI v1.20.0 is now the minimum requirement for kubevirt.&lt;/li&gt;
  &lt;li&gt;[PR #5846][rmohr] Add ‚Äúspec.cpu.numaTopologyPassthrough‚Äù which allows emulating a host-alligned virtual numa topology for high performance&lt;/li&gt;
  &lt;li&gt;[PR #5894][rmohr] Add &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;spec.migrations.disableTLS&lt;/code&gt; to the KubeVirt CR to allow disabling encrypted migrations. They stay secure by default.&lt;/li&gt;
  &lt;li&gt;[PR #5649][awels] Enhancement: remove one attachment pod per disk limit (behavior on upgrade with running VM with hotplugged disks is undefined)&lt;/li&gt;
  &lt;li&gt;[PR #5742][rmohr] VMIs which choose evictionStrategy &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;LifeMigrate&lt;/code&gt; and request the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;invtsc&lt;/code&gt; cpuflag are now live-migrateable&lt;/li&gt;
  &lt;li&gt;[PR #5911][dhiller] Bumps kubevirtci, also suppresses kubectl.sh output to avoid confusing checks&lt;/li&gt;
  &lt;li&gt;[PR #5863][xpivarc] Fix: ioerrors don‚Äôt cause crash-looping of notify server&lt;/li&gt;
  &lt;li&gt;[PR #5867][mlsorensen] New build target added to export virt-* images as a tar archive.&lt;/li&gt;
  &lt;li&gt;[PR #5766][davidvossel] Addition of kubevirt_vmi_phase_transition_seconds_since_creation to monitor how long it takes to transition a VMI to a specific phase from creation time.&lt;/li&gt;
  &lt;li&gt;[PR #5823][dhiller] Change default branch to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;main&lt;/code&gt; for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kubevirt/kubevirt&lt;/code&gt; repository&lt;/li&gt;
  &lt;li&gt;[PR #5763][nunnatsa] Fix bug 1945589: Prevent migration of VMIs that uses virtiofs&lt;/li&gt;
  &lt;li&gt;[PR #5827][mlsorensen] Auto-provisioned disk images on empty PVCs now leave 128KiB unused to avoid edge cases that run the volume out of space.&lt;/li&gt;
  &lt;li&gt;[PR #5849][davidvossel] Fixes event recording causing a segfault in virt-controller&lt;/li&gt;
  &lt;li&gt;[PR #5797][rhrazdil] Add serviceAccountDisk automatically when Istio is enabled in VMI annotations&lt;/li&gt;
  &lt;li&gt;[PR #5723][ashleyschuett] Allow virtctl to stop VM and ignore the graceful shutdown period&lt;/li&gt;
  &lt;li&gt;[PR #5806][mlsorensen] configmap, secret, and cloud-init raw disks now work when underlying node storage has 4k blocks.&lt;/li&gt;
  &lt;li&gt;[PR #5623][iholder-redhat] [bugfix]: Allow migration of VMs with host-model CPU to migrate only for compatible nodes&lt;/li&gt;
  &lt;li&gt;[PR #5716][rhrazdil] Fix issue with virt-launcher becoming &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NotReady&lt;/code&gt; after migration when Istio is used.&lt;/li&gt;
  &lt;li&gt;[PR #5778][ashleyschuett] Update ca-bundle if it is unable to be parsed&lt;/li&gt;
  &lt;li&gt;[PR #5787][acardace] migrated references of authorization/v1beta1 to authorization/v1&lt;/li&gt;
  &lt;li&gt;[PR #5461][rhrazdil] Add support for Istio proxy when no explicit ports are specified on masquerade interface&lt;/li&gt;
  &lt;li&gt;[PR #5751][acardace] EFI VMIs with secureboot disabled can now be booted even when only OVMF_CODE.secboot.fd and OVMF_VARS.fd are present in the virt-launcher image&lt;/li&gt;
  &lt;li&gt;[PR #5629][andreyod] Support starting Virtual Machine with its guest CPU paused using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;virtctl start --paused&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;[PR #5725][dhiller] Generate REST API coverage report after functional tests&lt;/li&gt;
  &lt;li&gt;[PR #5758][davidvossel] Fixes kubevirt_vmi_phase_count to include all phases, even those that occur before handler hand off.&lt;/li&gt;
  &lt;li&gt;[PR #5745][ashleyschuett] Alert with resource usage exceeds resource requests&lt;/li&gt;
  &lt;li&gt;[PR #5759][mhenriks] Update CDI to 1.34.1&lt;/li&gt;
  &lt;li&gt;[PR #5038][kwiesmueller] Add exec command to VM liveness and readinessProbe executed through the qemu-guest-agent.&lt;/li&gt;
  &lt;li&gt;[PR #5431][alonSadan] Add NFT and IPTables rules to allow port-forward to non-declared ports on the VMI. Declaring ports on VMI will limit&lt;/li&gt;
&lt;/ul&gt;</content><author><name>kubeü§ñ</name></author><category term="releases" /><category term="release notes" /><category term="changelog" /><summary type="html">v0.43.0</summary></entry><entry><title type="html">KubeVirt v0.42.0</title><link href="https://kubevirt.io//2021/changelog-v0.42.0.html" rel="alternate" type="text/html" title="KubeVirt v0.42.0" /><published>2021-06-08T00:00:00+00:00</published><updated>2021-06-08T00:00:00+00:00</updated><id>https://kubevirt.io//2021/changelog-v0.42.0</id><content type="html" xml:base="https://kubevirt.io//2021/changelog-v0.42.0.html">&lt;h2 id=&quot;v0420&quot;&gt;v0.42.0&lt;/h2&gt;

&lt;p&gt;Released on: Tue Jun 8 12:09:49 2021 +0000&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;[PR #5738][rmohr] Stop releasing jinja2 templates of our operator. Kustomize is the preferred way for customizations.&lt;/li&gt;
  &lt;li&gt;[PR #5691][ashleyschuett] Allow multiple shutdown events to ensure the event is received by ACPI&lt;/li&gt;
  &lt;li&gt;[PR #5558][ormergi] Drop virt-launcher SYS_RESOURCE capability&lt;/li&gt;
  &lt;li&gt;[PR #5694][davidvossel] Fixes null pointer dereference in migration controller&lt;/li&gt;
  &lt;li&gt;[PR #5416][iholder-redhat] [feature] support booting VMs from a custom kernel/initrd images with custom kernel arguments&lt;/li&gt;
  &lt;li&gt;[PR #5495][iholder-redhat] Go version updated to version 1.16.1.&lt;/li&gt;
  &lt;li&gt;[PR #5502][rmohr] Add downwardMetrics volume to expose a limited set of hots metrics to guests&lt;/li&gt;
  &lt;li&gt;[PR #5601][maya-r] Update libvirt-go to 7.3.0&lt;/li&gt;
  &lt;li&gt;[PR #5661][davidvossel] Validation/Mutation webhooks now explicitly define a 10 second timeout period&lt;/li&gt;
  &lt;li&gt;[PR #5652][rmohr] Automatically discover kube-prometheus installations and configure kubevirt monitoring&lt;/li&gt;
  &lt;li&gt;[PR #5631][davidvossel] Expand backport policy to include logging and debug fixes&lt;/li&gt;
  &lt;li&gt;[PR #5528][zcahana] Introduced a ‚Äústatus.printableStatus‚Äù field in the VirtualMachine CRD. This field is now displayed in the tabular output of ‚Äúkubectl get vm‚Äù.&lt;/li&gt;
  &lt;li&gt;[PR #5200][rhrazdil] Add support for Istio proxy traffic routing with masquerade interface. nftables is required for this feature.&lt;/li&gt;
  &lt;li&gt;[PR #5560][oshoval] virt-launcher now populates domain‚Äôs guestOS info and interfaces status according guest agent also when doing periodic resyncs.&lt;/li&gt;
  &lt;li&gt;[PR #5514][rhrazdil] Fix live-migration failing when VM with masquarade iface has explicitly specified any of these ports: 22222, 49152, 49153&lt;/li&gt;
  &lt;li&gt;[PR #5583][dhiller] Reenable coverage&lt;/li&gt;
  &lt;li&gt;[PR #5129][davidvossel] Gracefully shutdown virt-api connections and ensure zero exit code under normal shutdown conditions&lt;/li&gt;
  &lt;li&gt;[PR #5582][dhiller] Fix flaky unit tests&lt;/li&gt;
  &lt;li&gt;[PR #5600][davidvossel] Improved logging around VM/VMI shutdown and restart&lt;/li&gt;
  &lt;li&gt;[PR #5564][omeryahud] virtctl rename support is dropped&lt;/li&gt;
  &lt;li&gt;[PR #5585][iholder-redhat] [bugfix] - reject VM defined with volume with no matching disk&lt;/li&gt;
  &lt;li&gt;[PR #5595][zcahana] Fixes adoption of orphan DataVolumes&lt;/li&gt;
  &lt;li&gt;[PR #5566][davidvossel] Release branches are now cut on the first &lt;em&gt;business day&lt;/em&gt; of the month rather than the first day.&lt;/li&gt;
  &lt;li&gt;[PR #5108][Omar007] Fixes handling of /proc/&lt;pid&gt;/mountpoint by working on the device information instead of mount information&lt;/pid&gt;&lt;/li&gt;
  &lt;li&gt;[PR #5250][mlsorensen] Controller health checks will no longer actively test connectivity to the Kubernetes API. They will rely in health of their watches to determine if they have API connectivity.&lt;/li&gt;
  &lt;li&gt;[PR #5563][ashleyschuett] Set KubeVirt resources flags in the KubeVirt CR&lt;/li&gt;
  &lt;li&gt;[PR #5328][andreabolognani] This version of KubeVirt includes upgraded virtualization technology based on libvirt 7.0.0 and QEMU 5.2.0.&lt;/li&gt;
&lt;/ul&gt;</content><author><name>kubeü§ñ</name></author><category term="releases" /><category term="release notes" /><category term="changelog" /><summary type="html">v0.42.0</summary></entry><entry><title type="html">KubeVirt v0.41.0</title><link href="https://kubevirt.io//2021/changelog-v0.41.0.html" rel="alternate" type="text/html" title="KubeVirt v0.41.0" /><published>2021-05-12T00:00:00+00:00</published><updated>2021-05-12T00:00:00+00:00</updated><id>https://kubevirt.io//2021/changelog-v0.41.0</id><content type="html" xml:base="https://kubevirt.io//2021/changelog-v0.41.0.html">&lt;h2 id=&quot;v0410&quot;&gt;v0.41.0&lt;/h2&gt;

&lt;p&gt;Released on: Wed May 12 14:30:49 2021 +0000&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;[PR #5586][kubevirt-bot] This version of KubeVirt includes upgraded virtualization technology based on libvirt 7.0.0 and QEMU 5.2.0.&lt;/li&gt;
  &lt;li&gt;[PR #5344][ashleyschuett] Reconcile PrometheusRules and ServiceMonitor resources&lt;/li&gt;
  &lt;li&gt;[PR #5542][andreyod] Add startStrategy field to VMI spec to allow Virtual Machine start in paused state.&lt;/li&gt;
  &lt;li&gt;[PR #5459][ashleyschuett] Reconcile service resource&lt;/li&gt;
  &lt;li&gt;[PR #5520][ashleyschuett] Reconcile required labels and annotations on ConfigMap resources&lt;/li&gt;
  &lt;li&gt;[PR #5533][rmohr] Fix &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;docker save&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;docker push&lt;/code&gt; issues with released kubevirt images&lt;/li&gt;
  &lt;li&gt;[PR #5428][oshoval] virt-launcher now populates domain‚Äôs guestOS info and interfaces status according guest agent also when doing periodic resyncs.&lt;/li&gt;
  &lt;li&gt;[PR #5410][ashleyschuett] Reconcile ServiceAccount resources&lt;/li&gt;
  &lt;li&gt;[PR #5109][Omar007] Add support for specifying a logical and physical block size for disk devices&lt;/li&gt;
  &lt;li&gt;[PR #5471][ashleyschuett] Reconcile APIService resources&lt;/li&gt;
  &lt;li&gt;[PR #5513][ashleyschuett] Reconcile Secret resources&lt;/li&gt;
  &lt;li&gt;[PR #5496][davidvossel] Improvements to migration proxy logging&lt;/li&gt;
  &lt;li&gt;[PR #5376][ashleyschuett] Reconcile CustomResourceDefinition resources&lt;/li&gt;
  &lt;li&gt;[PR #5435][AlonaKaplan] Support dual stack service on ‚Äúvirtctl expose‚Äù-&lt;/li&gt;
  &lt;li&gt;[PR #5425][davidvossel] Fixes VM restart during eviction when EvictionStrategy=LiveMigrate&lt;/li&gt;
  &lt;li&gt;[PR #5423][ashleyschuett] Add resource requests to virt-controller, virt-api, virt-operator and virt-handler&lt;/li&gt;
  &lt;li&gt;[PR #5343][erkanerol] Some cleanups and small additions to the storage metrics&lt;/li&gt;
  &lt;li&gt;[PR #4682][stu-gott] Updated Guest Agent Version compatibility check. The new approach is much more accurate.&lt;/li&gt;
  &lt;li&gt;[PR #5485][rmohr] Fix fallback to iptables if nftables is not used on the host on arm64&lt;/li&gt;
  &lt;li&gt;[PR #5426][rmohr] Fix fallback to iptables if nftables is not used on the host&lt;/li&gt;
  &lt;li&gt;[PR #5403][tiraboschi] Added a kubevirt_ prefix to several recording rules and metrics&lt;/li&gt;
  &lt;li&gt;[PR #5241][stu-gott] Introduced Duration and RenewBefore parameters for cert rotation. Previous values are now deprecated.&lt;/li&gt;
  &lt;li&gt;[PR #5463][acardace] Fixes upgrades from KubeVirt v0.36&lt;/li&gt;
  &lt;li&gt;[PR #5456][zhlhahaha] Enable arm64 cross-compilation&lt;/li&gt;
  &lt;li&gt;[PR #3310][davidvossel] Doc outlines our Kubernetes version compatibility commitment&lt;/li&gt;
  &lt;li&gt;[PR #3383][EdDev] Add &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;vmIPv6NetworkCIDR&lt;/code&gt; under &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NetworkSource.pod&lt;/code&gt; to support custom IPv6 CIDR for the vm network when using masquerade binding.&lt;/li&gt;
  &lt;li&gt;[PR #3415][zhlhahaha] Make kubevirt code fit for arm64 support. No testing is at this stage performed against arm64 at this point.&lt;/li&gt;
  &lt;li&gt;[PR #5147][xpivarc] Remove CAP_NET_ADMIN from the virt-launcher pod(second take).&lt;/li&gt;
  &lt;li&gt;[PR #5351][awels] Support hotplug with virtctl using addvolume and removevolume commands&lt;/li&gt;
  &lt;li&gt;[PR #5050][ashleyschuett] Fire Prometheus Alert when a vmi is orphaned for more than an hour&lt;/li&gt;
&lt;/ul&gt;</content><author><name>kubeü§ñ</name></author><category term="releases" /><category term="release notes" /><category term="changelog" /><summary type="html">v0.41.0</summary></entry><entry><title type="html">Using Intel vGPUs with Kubevirt</title><link href="https://kubevirt.io//2021/intel-vgpu-kubevirt.html" rel="alternate" type="text/html" title="Using Intel vGPUs with Kubevirt" /><published>2021-04-30T00:00:00+00:00</published><updated>2021-04-30T00:00:00+00:00</updated><id>https://kubevirt.io//2021/intel-vgpu-kubevirt</id><content type="html" xml:base="https://kubevirt.io//2021/intel-vgpu-kubevirt.html">&lt;!-- TOC depthFrom:2 insertAnchor:false orderedList:false updateOnSave:true withLinks:true --&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#introduction&quot;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#prerequisites&quot;&gt;Prerequisites&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#fedora-workstation-prep&quot;&gt;Fedora Workstation Prep&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#preparing-the-intel-vgpu-driver&quot;&gt;Preparing the Intel vGPU driver&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#install-kubernetes-with-minikube&quot;&gt;Install Kubernetes with minikube&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#install-kubevirt&quot;&gt;Install kubevirt&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#validate-vgpu-detection&quot;&gt;Validate vGPU detection&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#install-containerize-data-importer&quot;&gt;Install Containerize Data Importer&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#install-windows&quot;&gt;Install Windows&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#accessing-the-windows-vm&quot;&gt;Accessing the Windows VM&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#using-the-gpu&quot;&gt;Using the GPU&lt;/a&gt;
&lt;!-- /TOC --&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;Graphical User Interfaces (GUIs) have come along way over the past few years and most modern desktop environments expect some form of GPU acceleration in order to give you a seamless user experience. If you have tried running things like Windows 10 within Kubevirt you may have noticed that the desktop experience felt a little slow. This is due to Windows 10 reliance on GPU acceleration. In addition many applications are also now taking advantage of GPU acceleration and it can even be used in web based applications such as ‚ÄúFishGL‚Äù:
&lt;br /&gt;&lt;/p&gt;

&lt;div class=&quot;my-gallery&quot; itemscope=&quot;&quot; itemtype=&quot;http://schema.org/ImageGallery&quot;&gt;
  &lt;figure itemprop=&quot;associatedMedia&quot; itemscope=&quot;&quot; itemtype=&quot;http://schema.org/ImageObject&quot;&gt;
    &lt;a href=&quot;/assets/2021-04-30-intel-vgpu-kubevirt/fishgl-nogpu.png&quot; itemprop=&quot;contentUrl&quot; data-size=&quot;800x530&quot;&gt;
      &lt;img src=&quot;/assets/2021-04-30-intel-vgpu-kubevirt/fishgl-nogpu.png&quot; itemprop=&quot;thumbnail&quot; width=&quot;100%&quot; alt=&quot;FishGL&quot; /&gt;
    &lt;/a&gt;
    &lt;figcaption itemprop=&quot;caption description&quot;&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Without GPU hardware acceleration the user experience of a Virtual machine can be greatly impacted.&lt;/p&gt;

&lt;p&gt;Starting with 5th generation Intel Core processors that have embedded Intel graphics processing units it is possible to share the graphics processor between multiple virtual machines. In Linux, this sharing of a GPU is typically enabled through the use of mediated GPU devices, also known as vGPUs. Kubevirt has supported the use of GPUs including GPU passthrough and vGPU since v0.22.0 back in 2019. This support was centered around one specific vendor, and only worked with expensive enterprise class cards and required additional licensing. Starting with &lt;a href=&quot;https://github.com/kubevirt/kubevirt/releases/tag/v0.40.0&quot;&gt;Kubevirt 0.40&lt;/a&gt; support for detecting and allocating the Intel based vGPUs has been added to Kubevirt. Support for the creation of these virtualized Intel GPUs is available in the Linux Kernel since the 4.19 release. What does this meaning for you? You no longer need additional drivers or licenses to test out GPU accelerated virtual machines.&lt;/p&gt;

&lt;p&gt;The total number of Intel vGPUs you can create is dependent on your specific hardware as well as support for changing the Graphics aperture size and shared graphics memory within your BIOS. For more details on this see &lt;a href=&quot;https://github.com/intel/gvt-linux/wiki/GVTg_Setup_Guide#53-create-vgpu-kvmgt-only&quot;&gt;Create vGPU (KVMGT only)&lt;/a&gt; in the Intel GVTg wiki. Minimally configured devices can typically make at least two vGPU devices.&lt;/p&gt;

&lt;p&gt;You can reproduce this work on any Kubernetes cluster running kubevirt v0.40.0 or later, but the steps you need to take to load the kernel modules and enable the virtual devices will vary based on the underlying OS your Kubernetes cluster is running on. In order to demonstrate how you can enable this feature, we will use an all-in-one Kubernetes cluster built using Fedora 32 and minikube.&lt;/p&gt;

&lt;div class=&quot;premonition note&quot;&gt;&lt;div class=&quot;fa fa-check-square&quot;&gt;&lt;/div&gt;&lt;div class=&quot;content&quot;&gt;&lt;p class=&quot;header&quot;&gt;Note&lt;/p&gt;&lt;p&gt;This blog post is a more advanced topic and assumes some Linux and Kubernetes understanding.&lt;/p&gt;


&lt;/div&gt;&lt;/div&gt;
&lt;h2 id=&quot;prerequisites&quot;&gt;Prerequisites&lt;/h2&gt;

&lt;p&gt;Before we begin you will need a few things to make use of the Intel GPU:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;A workstation or server with a 5th Generation or higher Intel Core Processor, or E3_v4 or higher Xeon Processor and enough memory to virtualize one or more VMs&lt;/li&gt;
  &lt;li&gt;A preinstalled Fedora 32 Workstation with at least 50Gb of free space in the ‚Äú/‚Äù filesystem&lt;/li&gt;
  &lt;li&gt;The following software:
    &lt;ul&gt;
      &lt;li&gt;minikube - See &lt;a href=&quot;https://minikube.sigs.k8s.io/docs/start/&quot;&gt;minikube start&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;virtctl - See &lt;a href=&quot;https://github.com/kubevirt/kubevirt/releases&quot;&gt;kubevirt releases&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;kubectl - See &lt;a href=&quot;https://kubernetes.io/docs/tasks/tools/install-kubectl-linux/&quot;&gt;Install and Set Up kubectl on Linux&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;A Windows 10 Install ISO Image - See &lt;a href=&quot;https://www.microsoft.com/en-us/software-download/windows10&quot;&gt;Download Windows 10 Disk Image&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;fedora-workstation-prep&quot;&gt;Fedora Workstation Prep&lt;/h3&gt;

&lt;p&gt;In order to use minikube on Fedora 32 we will be installing multiple applications that will be used throughout this demo. In addition we will be configuring the workstation to use cgroups v1 and we will be updating the firewall to allow proper communication to our Kubernetes cluster as well as any hosted applications. Finally we will be disabling SELinux per the minikube bare-metal install instructions:&lt;/p&gt;

&lt;div class=&quot;premonition note&quot;&gt;&lt;div class=&quot;fa fa-check-square&quot;&gt;&lt;/div&gt;&lt;div class=&quot;content&quot;&gt;&lt;p class=&quot;header&quot;&gt;Note&lt;/p&gt;&lt;p&gt;This post assumes that we are starting with a fresh install of Fedora 32. If you are using an existing configured Fedora 32 Workstation, you may have some software conflicts.&lt;/p&gt;


&lt;/div&gt;&lt;/div&gt;
&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;dnf update &lt;span class=&quot;nt&quot;&gt;-y&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;dnf &lt;span class=&quot;nb&quot;&gt;install&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-y&lt;/span&gt; pciutils podman podman-docker conntrack tigervnc rdesktop
&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;grubby &lt;span class=&quot;nt&quot;&gt;--update-kernel&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;ALL &lt;span class=&quot;nt&quot;&gt;--args&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;systemd.unified_cgroup_hierarchy=0&quot;&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# Setup firewall rules to allow inbound and outbound connections from your minikube cluster&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;firewall-cmd &lt;span class=&quot;nt&quot;&gt;--add-port&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;30000-65535/tcp &lt;span class=&quot;nt&quot;&gt;--permanent&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;firewall-cmd &lt;span class=&quot;nt&quot;&gt;--add-port&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;30000-65535/udp &lt;span class=&quot;nt&quot;&gt;--permanent&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;firewall-cmd &lt;span class=&quot;nt&quot;&gt;--add-port&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;10250-10252/tcp &lt;span class=&quot;nt&quot;&gt;--permanent&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;firewall-cmd &lt;span class=&quot;nt&quot;&gt;--add-port&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;10248/tcp &lt;span class=&quot;nt&quot;&gt;--permanent&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;firewall-cmd &lt;span class=&quot;nt&quot;&gt;--add-port&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;2379-2380/tcp &lt;span class=&quot;nt&quot;&gt;--permanent&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;firewall-cmd &lt;span class=&quot;nt&quot;&gt;--add-port&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;6443/tcp &lt;span class=&quot;nt&quot;&gt;--permanent&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;firewall-cmd &lt;span class=&quot;nt&quot;&gt;--add-port&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;8443/tcp &lt;span class=&quot;nt&quot;&gt;--permanent&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;firewall-cmd &lt;span class=&quot;nt&quot;&gt;--add-port&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;9153/tcp &lt;span class=&quot;nt&quot;&gt;--permanent&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;firewall-cmd &lt;span class=&quot;nt&quot;&gt;--add-service&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;dns &lt;span class=&quot;nt&quot;&gt;--permanent&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;firewall-cmd &lt;span class=&quot;nt&quot;&gt;--add-interface&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;cni-podman0 &lt;span class=&quot;nt&quot;&gt;--permanent&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;firewall-cmd &lt;span class=&quot;nt&quot;&gt;--add-masquerade&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--permanent&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;vi /etc/selinux/config
&lt;span class=&quot;c&quot;&gt;# change the &quot;SELINUX=enforcing&quot; to &quot;SELINUX=permissive&quot;&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;setenforce 0
&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;systemctl &lt;span class=&quot;nb&quot;&gt;enable &lt;/span&gt;sshd &lt;span class=&quot;nt&quot;&gt;--now&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We will now install the CRIO runtime:&lt;/p&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;dnf module &lt;span class=&quot;nb&quot;&gt;enable&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-y&lt;/span&gt; cri-o:1.18
&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;dnf &lt;span class=&quot;nb&quot;&gt;install&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-y&lt;/span&gt; cri-o cri-tools
&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;systemctl &lt;span class=&quot;nb&quot;&gt;enable&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--now&lt;/span&gt; crio
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;preparing-the-intel-vgpu-driver&quot;&gt;Preparing the Intel vGPU driver&lt;/h3&gt;

&lt;p&gt;In order to make use of the Intel vGPU driver, we need to make a few changes to our all-in-one host. The commands below assume you are using a Fedora based host. If you are using a different base OS, be sure to update your commands for that specific distribution.&lt;/p&gt;

&lt;p&gt;The following commands will do the following:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;load the kvmgt module to enable support within kvm&lt;/li&gt;
  &lt;li&gt;enable gvt in the i915 module&lt;/li&gt;
  &lt;li&gt;update the Linux kernel to enable Intel IOMMU&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;sh &lt;span class=&quot;nt&quot;&gt;-c&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;echo kvmgt &amp;gt; /etc/modules-load.d/gpu-kvmgt.conf&quot;&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;grubby &lt;span class=&quot;nt&quot;&gt;--update-kernel&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;ALL &lt;span class=&quot;nt&quot;&gt;--args&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;intel_iommu=on i915.enable_gvt=1&quot;&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;shutdown &lt;span class=&quot;nt&quot;&gt;-r&lt;/span&gt; now
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;After the reboot check to ensure that the proper kernel modules have been loaded:&lt;/p&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;lsmod | &lt;span class=&quot;nb&quot;&gt;grep &lt;/span&gt;kvmgt
kvmgt                  32768  0
mdev                   20480  2 kvmgt,vfio_mdev
vfio                   32768  3 kvmgt,vfio_mdev,vfio_iommu_type1
kvm                   798720  2 kvmgt,kvm_intel
i915                 2494464  4 kvmgt
drm                   557056  4 drm_kms_helper,kvmgt,i915
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We will now create our vGPU devices. These virtual devices are created by echoing a GUID into a sys device created by the Intel driver. This needs to be done every time the system boots. The easiest way to do this is using a systemd service that runs on every boot. Before we create this systemd service, we need to validate the PCI ID of your Intel Graphics card. To do this we will use the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;lspci&lt;/code&gt; command&lt;/p&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;lspci
00:00.0 Host bridge: Intel Corporation Device 9b53 &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;rev 03&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
00:02.0 VGA compatible controller: Intel Corporation Device 9bc8 &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;rev 03&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
00:08.0 System peripheral: Intel Corporation Xeon E3-1200 v5/v6 / E3-1500 v5 / 6th/7th/8th Gen Core Processor Gaussian Mixture Model
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Take note that in the above output the Intel GPU is on ‚Äú00:02.0‚Äù. Now create the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/etc/systemd/system/gvtg-enable.service&lt;/code&gt; but be sure to update the PCI ID as appropriate for your machine:&lt;/p&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;cat&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; ~/gvtg-enable.service &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;EOF&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;
[Unit]
Description=Create Intel GVT-g vGPU

[Service]
Type=oneshot
ExecStart=/bin/sh -c &quot;echo '56a4c4e2-c81f-4cba-82bf-af46c30ea32d' &amp;gt; /sys/devices/pci0000:00/0000:00:02.0/mdev_supported_types/i915-GVTg_V5_8/create&quot;
ExecStart=/bin/sh -c &quot;echo '973069b7-2025-406b-b3c9-301016af3150' &amp;gt; /sys/devices/pci0000:00/0000:00:02.0/mdev_supported_types/i915-GVTg_V5_8/create&quot;
ExecStop=/bin/sh -c &quot;echo '1' &amp;gt; /sys/devices/pci0000:00/0000:00:02.0/56a4c4e2-c81f-4cba-82bf-af46c30ea32d/remove&quot;
ExecStop=/bin/sh -c &quot;echo '1' &amp;gt; /sys/devices/pci0000:00/0000:00:02.0/973069b7-2025-406b-b3c9-301016af3150/remove&quot;
RemainAfterExit=yes

[Install]
WantedBy=multi-user.target
&lt;/span&gt;&lt;span class=&quot;no&quot;&gt;EOF
&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sudo mv&lt;/span&gt; ~/gvtg-enable.service /etc/systemd/system/gvtg-enable.service
&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;systemctl &lt;span class=&quot;nb&quot;&gt;enable &lt;/span&gt;gvtg-enable &lt;span class=&quot;nt&quot;&gt;--now&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;premonition note&quot;&gt;&lt;div class=&quot;fa fa-check-square&quot;&gt;&lt;/div&gt;&lt;div class=&quot;content&quot;&gt;&lt;p class=&quot;header&quot;&gt;Note&lt;/p&gt;&lt;p&gt;The above systemd service will create two vGPU devices, you can repeat the commands with additional unique GUIDs up to a maximum of 8 vGPU if your particular hardware supports it.&lt;/p&gt;


&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;We can validate that the vGPU devices were created by looking in the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/sys/devices/pci0000:00/0000:00:02.0/&lt;/code&gt; directory.&lt;/p&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;ls&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-lsa&lt;/span&gt; /sys/devices/pci0000:00/0000:00:02.0/56a4c4e2-c81f-4cba-82bf-af46c30ea32d
total 0
lrwxrwxrwx. 1 root root    0 Apr 20 13:56 driver -&amp;gt; ../../../../bus/mdev/drivers/vfio_mdev
drwxr-xr-x. 2 root root    0 Apr 20 14:41 intel_vgpu
lrwxrwxrwx. 1 root root    0 Apr 20 14:41 iommu_group -&amp;gt; ../../../../kernel/iommu_groups/8
lrwxrwxrwx. 1 root root    0 Apr 20 14:41 mdev_type -&amp;gt; ../mdev_supported_types/i915-GVTg_V5_8
drwxr-xr-x. 2 root root    0 Apr 20 14:41 power
&lt;span class=&quot;nt&quot;&gt;--w-------&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;.&lt;/span&gt; 1 root root 4096 Apr 20 14:41 remove
lrwxrwxrwx. 1 root root    0 Apr 20 13:56 subsystem -&amp;gt; ../../../../bus/mdev
&lt;span class=&quot;nt&quot;&gt;-rw-r--r--&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;.&lt;/span&gt; 1 root root 4096 Apr 20 13:56 uevent
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Note that ‚Äúmdev_type‚Äù points to ‚Äúi915-GVTg_V5_8‚Äù, this will come into play later when we configure kubevirt to detect the vGPU.&lt;/p&gt;

&lt;h2 id=&quot;install-kubernetes-with-minikube&quot;&gt;Install Kubernetes with minikube&lt;/h2&gt;

&lt;p&gt;We will now install Kubernetes onto our Fedora Workstation. &lt;a href=&quot;https://minikube.sigs.k8s.io/docs/&quot;&gt;Minikube&lt;/a&gt; will help quickly set up our Kubernetes cluster environment. We will start by getting the latest release of minikube and kubectl.&lt;/p&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;curl &lt;span class=&quot;nt&quot;&gt;-LO&lt;/span&gt; https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64
&lt;span class=&quot;nb&quot;&gt;sudo install &lt;/span&gt;minikube-linux-amd64 /usr/local/bin/minikube
&lt;span class=&quot;nv&quot;&gt;VERSION&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;$(&lt;/span&gt;minikube kubectl version | &lt;span class=&quot;nb&quot;&gt;head&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-1&lt;/span&gt; | &lt;span class=&quot;nb&quot;&gt;awk&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-F&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;', '&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'print $3'&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt; | &lt;span class=&quot;nb&quot;&gt;awk&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-F&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;':'&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'print $2'&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt; | &lt;span class=&quot;nb&quot;&gt;sed &lt;/span&gt;s/&lt;span class=&quot;se&quot;&gt;\&quot;&lt;/span&gt;//g&lt;span class=&quot;si&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;sudo install&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;HOME&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;/.minikube/cache/linux/&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;VERSION&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;/kubectl /usr/local/bin
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We will be using the minikube driver ‚Äúnone‚Äù which will install Kubernetes directly onto this machine. This will allow you to maintain a copy of the virtual machines that you build through a reboot. Later in this post we will create persistent volumes for virtual machine storage in ‚Äú/data‚Äù. As previously noted, ensure that you have at least 50Gb of free space in ‚Äú/data‚Äù to complete this setup. The minikube install will take a few minutes to complete.&lt;/p&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sudo mkdir&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-p&lt;/span&gt; /data/winhd1-pv
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;minikube start &lt;span class=&quot;nt&quot;&gt;--driver&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;none &lt;span class=&quot;nt&quot;&gt;--container-runtime&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;crio
üòÑ  minikube v1.19.0 on Fedora 32
‚ú®  Using the none driver based on user configuration
üëç  Starting control plane node minikube &lt;span class=&quot;k&quot;&gt;in &lt;/span&gt;cluster minikube
ü§π  Running on localhost &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;CPUs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;12, &lt;span class=&quot;nv&quot;&gt;Memory&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;31703MB, &lt;span class=&quot;nv&quot;&gt;Disk&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;71645MB&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; ...
‚ÑπÔ∏è  OS release is Fedora 32 &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;Workstation Edition&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
üê≥  Preparing Kubernetes v1.20.2 on Docker 20.10.6 ...
    ‚ñ™ Generating certificates and keys ...
    ‚ñ™ Booting up control plane ...
    ‚ñ™ Configuring RBAC rules ...
ü§π  Configuring &lt;span class=&quot;nb&quot;&gt;local &lt;/span&gt;host environment ...
üîé  Verifying Kubernetes components...
    ‚ñ™ Using image gcr.io/k8s-minikube/storage-provisioner:v5
üåü  Enabled addons: storage-provisioner, default-storageclass
üèÑ  Done! kubectl is now configured to use &lt;span class=&quot;s2&quot;&gt;&quot;minikube&quot;&lt;/span&gt; cluster and &lt;span class=&quot;s2&quot;&gt;&quot;default&quot;&lt;/span&gt; namespace by default
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In order to make our interaction with Kubernetes a little easier, we will need to copy some files and update our &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;.kube/config&lt;/code&gt;&lt;/p&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;mkdir&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-p&lt;/span&gt; ~/.minikube/profiles/minikube
&lt;span class=&quot;nb&quot;&gt;sudo cp&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-r&lt;/span&gt; /root/.kube /home/&lt;span class=&quot;nv&quot;&gt;$USER&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;sudo cp&lt;/span&gt; /root/.minikube/ca.crt /home/&lt;span class=&quot;nv&quot;&gt;$USER&lt;/span&gt;/.minikube/ca.crt
&lt;span class=&quot;nb&quot;&gt;sudo cp&lt;/span&gt; /root/.minikube/profiles/minikube/client.crt /home/&lt;span class=&quot;nv&quot;&gt;$USER&lt;/span&gt;/.minikube/profiles/minikube
&lt;span class=&quot;nb&quot;&gt;sudo cp&lt;/span&gt; /root/.minikube/profiles/minikube/client.key /home/&lt;span class=&quot;nv&quot;&gt;$USER&lt;/span&gt;/.minikube/profiles/minikube
&lt;span class=&quot;nb&quot;&gt;sudo chown&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-R&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$USER&lt;/span&gt;:&lt;span class=&quot;nv&quot;&gt;$USER&lt;/span&gt; /home/&lt;span class=&quot;nv&quot;&gt;$USER&lt;/span&gt;/.kube
&lt;span class=&quot;nb&quot;&gt;sudo chown&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-R&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$USER&lt;/span&gt;:&lt;span class=&quot;nv&quot;&gt;$USER&lt;/span&gt; /home/&lt;span class=&quot;nv&quot;&gt;$USER&lt;/span&gt;/.minikube
&lt;span class=&quot;nb&quot;&gt;sed&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-i&lt;/span&gt;  &lt;span class=&quot;s2&quot;&gt;&quot;s/root/home&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\/&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$USER&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;/&quot;&lt;/span&gt; ~/.kube/config
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Once the minikube install is complete, validate that everything is working properly.&lt;/p&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl get nodes
NAME       STATUS   ROLES                  AGE    VERSION
kubevirt   Ready    control-plane,master   4m5s   v1.20.2
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;As long as you don‚Äôt get any errors, your base Kubernetes cluster is ready to go.&lt;/p&gt;

&lt;h2 id=&quot;install-kubevirt&quot;&gt;Install kubevirt&lt;/h2&gt;

&lt;p&gt;Our all-in-one Kubernetes cluster is now ready for installing Installing Kubevirt. Using the minikube addons manager, we will install kubevirt into our cluster:&lt;/p&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;minikube addons &lt;span class=&quot;nb&quot;&gt;enable &lt;/span&gt;kubevirt
kubectl &lt;span class=&quot;nt&quot;&gt;-n&lt;/span&gt; kubevirt &lt;span class=&quot;nb&quot;&gt;wait &lt;/span&gt;kubevirt kubevirt &lt;span class=&quot;nt&quot;&gt;--for&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;condition&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;Available &lt;span class=&quot;nt&quot;&gt;--timeout&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;300s
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;At this point, we need to update our instance of kubevirt in the cluster. We need to configure kubevirt to detect the Intel vGPU by giving it an &lt;em&gt;mdevNameSelector&lt;/em&gt; to look for, and a &lt;em&gt;resourceName&lt;/em&gt; to assign to it. The &lt;em&gt;mdevNameSelector&lt;/em&gt; comes from the ‚Äúmdev_type‚Äù that we identified earlier when we created the two virtual GPUs. When the kubevirt device manager finds instances of this mdev type, it will record this information and tag the node with the identified resourceName. We will use this resourceName later when we start up our virtual machine.&lt;/p&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;cat&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; kubevirt-patch.yaml &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;EOF&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;
spec:
  configuration:
    developerConfiguration:
      featureGates:
      - GPU
    permittedHostDevices:
      mediatedDevices:
      - mdevNameSelector: &quot;i915-GVTg_V5_8&quot;
        resourceName: &quot;intel.com/U630&quot;
&lt;/span&gt;&lt;span class=&quot;no&quot;&gt;EOF
&lt;/span&gt;kubectl patch kubevirt kubevirt &lt;span class=&quot;nt&quot;&gt;-n&lt;/span&gt; kubevirt &lt;span class=&quot;nt&quot;&gt;--patch&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;$(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;cat &lt;/span&gt;kubevirt-patch.yaml&lt;span class=&quot;si&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--type&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;merge
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We now need to wait for kubevirt to reload its configuration.&lt;/p&gt;

&lt;h3 id=&quot;validate-vgpu-detection&quot;&gt;Validate vGPU detection&lt;/h3&gt;

&lt;p&gt;Now that kubevirt is installed and running, lets ensure that the vGPU was identified correctly. Describe the minikube node, using the command &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kubectl describe node&lt;/code&gt; and look for the ‚ÄúCapacity‚Äù section. If kubevirt properly detected the vGPU you will see an entry for ‚Äúintel.com/U630‚Äù with a capacity value of greater than 0.&lt;/p&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl describe node
Name:               kubevirt
Roles:              control-plane,master
Labels:             beta.kubernetes.io/arch&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;amd64
                    beta.kubernetes.io/os&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;linux
...
Capacity:
  cpu:                            12
  devices.kubevirt.io/kvm:        110
  devices.kubevirt.io/tun:        110
  devices.kubevirt.io/vhost-net:  110
  ephemeral-storage:              71645Mi
  hugepages-1Gi:                  0
  hugepages-2Mi:                  0
  intel.com/U630:                 2
  memory:                         11822640Ki
  pods:                           110
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;There it is, intel.com/U630 - two of them are available.  Now all we need is a virtual machine to consume them.&lt;/p&gt;

&lt;h3 id=&quot;install-containerize-data-importer&quot;&gt;Install Containerize Data Importer&lt;/h3&gt;

&lt;p&gt;In order to install Windows 10, we are going to need to upload a Windows 10 install ISO to the cluster. This can be facilitated through the use of the Containerized Data Importer. The following steps are taken from the &lt;a href=&quot;https://kubevirt.io/labs/kubernetes/lab2.html&quot;&gt;Experiment with the Containerized Data Importer (CDI)&lt;/a&gt; web page:&lt;/p&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;export &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;VERSION&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;$(&lt;/span&gt;curl &lt;span class=&quot;nt&quot;&gt;-s&lt;/span&gt; https://github.com/kubevirt/containerized-data-importer/releases/latest | &lt;span class=&quot;nb&quot;&gt;grep&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-o&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;v[0-9]&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\.&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;[0-9]*&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\.&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;[0-9]*&quot;&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;)&lt;/span&gt;
kubectl create &lt;span class=&quot;nt&quot;&gt;-f&lt;/span&gt; https://github.com/kubevirt/containerized-data-importer/releases/download/&lt;span class=&quot;nv&quot;&gt;$VERSION&lt;/span&gt;/cdi-operator.yaml
kubectl create &lt;span class=&quot;nt&quot;&gt;-f&lt;/span&gt; https://github.com/kubevirt/containerized-data-importer/releases/download/&lt;span class=&quot;nv&quot;&gt;$VERSION&lt;/span&gt;/cdi-cr.yaml
kubectl &lt;span class=&quot;nt&quot;&gt;-n&lt;/span&gt; cdi &lt;span class=&quot;nb&quot;&gt;wait &lt;/span&gt;cdi cdi &lt;span class=&quot;nt&quot;&gt;--for&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;condition&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;Available &lt;span class=&quot;nt&quot;&gt;--timeout&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;300s
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Now that our CDI is available, we will expose it for consumption using a nodePort. This will allow us to connect to the cdi-proxy in the next steps.&lt;/p&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;cat&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; cdi-nodeport.yaml &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;EOF&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;
apiVersion: v1
kind: Service
metadata:
  name: cdi-proxy-nodeport
  namespace: cdi
spec:
  type: NodePort
  selector:
    cdi.kubevirt.io: cdi-uploadproxy
  ports:
    - port: 8443
      nodePort: 30443
&lt;/span&gt;&lt;span class=&quot;no&quot;&gt;EOF
&lt;/span&gt;kubectl create &lt;span class=&quot;nt&quot;&gt;-f&lt;/span&gt; cdi-nodeport.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;One final step, lets get the latest release of virtctl which we will be using as we install Windows.&lt;/p&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;VERSION&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;$(&lt;/span&gt;kubectl get kubevirt.kubevirt.io/kubevirt &lt;span class=&quot;nt&quot;&gt;-n&lt;/span&gt; kubevirt &lt;span class=&quot;nt&quot;&gt;-o&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;jsonpath&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;{.status.observedKubeVirtVersion}&quot;&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;)&lt;/span&gt;
curl &lt;span class=&quot;nt&quot;&gt;-L&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-o&lt;/span&gt; virtctl https://github.com/kubevirt/kubevirt/releases/download/&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;VERSION&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;/virtctl-&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;VERSION&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;-linux-amd64&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;sudo install &lt;/span&gt;virtctl /usr/local/bin
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;install-windows&quot;&gt;Install Windows&lt;/h2&gt;

&lt;p&gt;At this point we can now install a Windows VM in order to test this feature. The steps below are based on &lt;a href=&quot;https://kubevirt.io/2020/KubeVirt-installing_Microsoft_Windows_from_an_iso.html&quot;&gt;KubeVirt: installing Microsoft Windows from an ISO&lt;/a&gt; however we will be using Windows 10 instead of Windows Server 2012. The commands below assume that you have a Windows 10 ISO file called &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;win10-virtio.iso&lt;/code&gt;. If you need a Windows 10 CD, please see &lt;a href=&quot;https://www.microsoft.com/en-us/software-download/windows10&quot;&gt;Download Windows 10 Disk Image&lt;/a&gt; and come back here after you have obtained your install CD.&lt;/p&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;virtctl image-upload &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
   &lt;span class=&quot;nt&quot;&gt;--image-path&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;win10-virtio.iso &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
   &lt;span class=&quot;nt&quot;&gt;--pvc-name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;iso-win10 &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
   &lt;span class=&quot;nt&quot;&gt;--access-mode&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;ReadWriteOnce &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
   &lt;span class=&quot;nt&quot;&gt;--pvc-size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;6G &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
   &lt;span class=&quot;nt&quot;&gt;--uploadproxy-url&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;https://127.0.0.1:30443 &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
   &lt;span class=&quot;nt&quot;&gt;--insecure&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
   &lt;span class=&quot;nt&quot;&gt;--wait-secs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;240
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We need a place to store our Windows 10 virtual disk, use the following to create a 40Gb space to store our file. In order to do this within minikube we will manually create a PersistentVolume (PV) as well as a PersistentVolumeClaim (PVC). These steps assume that you have 45+ GiB of free space in ‚Äú/‚Äù.  We will create a ‚Äú/data‚Äù directory as well as a subdirectory for storing our PV. If you do not have at least 45 GiB of free space in ‚Äú/‚Äù, you will need to free up space, or mount storage on ‚Äú/data‚Äù to continue.&lt;/p&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;cat&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; win10-pvc.yaml &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;EOF&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;
---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pvwinhd1
spec:
  accessModes:
    - ReadWriteOnce
  capacity:
    storage: 43Gi
  claimRef:
    namespace: default
    name: winhd1
  hostPath:
    path: /data/winhd1-pv
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: winhd1
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 40Gi
&lt;/span&gt;&lt;span class=&quot;no&quot;&gt;EOF
&lt;/span&gt;kubectl create &lt;span class=&quot;nt&quot;&gt;-f&lt;/span&gt; win10-pvc.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We can now create our Windows 10 virtual machine. Use the following to create a virtual machine definition file that includes a vGPU:&lt;/p&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;cat&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; win10vm1.yaml &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;EOF&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;
apiVersion: kubevirt.io/v1alpha3
kind: VirtualMachine
metadata:
  name: win10vm1
spec:
  running: false
  template:
    metadata:
      creationTimestamp: null
      labels:
        kubevirt.io/domain: win10vm1
    spec:
      domain:
        clock:
          timer:
            hpet:
              present: false
            hyperv: {}
            pit:
              tickPolicy: delay
            rtc:
              tickPolicy: catchup
          utc: {}
        cpu:
          cores: 1
          sockets: 2
          threads: 1
        devices:
          gpus:
          - deviceName: intel.com/U630
            name: gpu1
          disks:
          - cdrom:
              bus: sata
            name: windows-guest-tools
          - bootOrder: 1
            cdrom:
              bus: sata
            name: cdrom
          - bootOrder: 2
            disk:
              bus: sata
            name: disk-1
          inputs:
          - bus: usb
            name: tablet
            type: tablet
          interfaces:
          - masquerade: {}
            model: e1000e
            name: nic-0
        features:
          acpi: {}
          apic: {}
          hyperv:
            relaxed: {}
            spinlocks:
              spinlocks: 8191
            vapic: {}
        machine:
          type: pc-q35-rhel8.2.0
        resources:
          requests:
            memory: 8Gi
      hostname: win10vm1
      networks:
      - name: nic-0
        pod: {}
      terminationGracePeriodSeconds: 3600
      volumes:
        - name: cdrom
          persistentVolumeClaim:
            claimName: iso-win10
        - name: disk-1
          persistentVolumeClaim:
            claimName: winhd1
        - containerDisk:
            image: quay.io/kubevirt/virtio-container-disk
          name: windows-guest-tools
&lt;/span&gt;&lt;span class=&quot;no&quot;&gt;EOF
&lt;/span&gt;kubectl create &lt;span class=&quot;nt&quot;&gt;-f&lt;/span&gt; win10vm1.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;premonition note&quot;&gt;&lt;div class=&quot;fa fa-check-square&quot;&gt;&lt;/div&gt;&lt;div class=&quot;content&quot;&gt;&lt;p class=&quot;header&quot;&gt;NOTE&lt;/p&gt;&lt;p&gt;This VM is not optimized to use virtio devices to simplify the OS install. By using SATA devices as well as an emulated e1000 network card, we do not need to worry about loading additional drivers.&lt;/p&gt;


&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;The key piece of information that we have added to this virtual machine definition is this snippet of yaml:&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;        &lt;span class=&quot;na&quot;&gt;devices&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
          &lt;span class=&quot;na&quot;&gt;gpus&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
          &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;deviceName&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;intel.com/U630&lt;/span&gt;
            &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;gpu1&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Here we are identifying the gpu device that we want to attach to this VM. The deviceName relates back to the name that we gave to kubevirt to identify the Intel GPU resources. It also is the same identifier that shows up in the ‚ÄúCapacity‚Äù section of a node when you run &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kubectl describe node&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;We can now start the virtual machine:&lt;/p&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;virtctl start win10vm1
kubectl get vmi &lt;span class=&quot;nt&quot;&gt;--watch&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;When the output of shows that the vm is in a ‚ÄúRunning‚Äù phase you can ‚ÄúCTRL+C‚Äù to end the watch command.&lt;/p&gt;

&lt;h2 id=&quot;accessing-the-windows-vm&quot;&gt;Accessing the Windows VM&lt;/h2&gt;

&lt;p&gt;Since we are running this VM on this local machine, we can now take advantage of the virtctl command to connect to the VNC console of the virtual machine.&lt;/p&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;virtctl vnc win10vm1
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;A new VNC Viewer window will open and you should now see the Windows 10 install screen. Follow standard Windows 10 install steps at this point.&lt;/p&gt;

&lt;p&gt;Once the install is complete you have a Windows 10 VM running with a GPU available. You can test that GPU acceleration is available by opening the Windows 10 task manager, selecting Advanced and then select the ‚ÄúPerformance‚Äù tab. Note that the first time you start up, Windows is still detecting and installing the appropriate drivers. It may take a minute or two for the GPU information to show up in the Performance tab.&lt;/p&gt;

&lt;p&gt;Try testing out the GPU acceleration. Open a web browser in your VM and navigate to ‚Äúhttps://webglsamples.org/fishtank/fishtank.html‚Äù HOWEVER don‚Äôt be surprised by the poor performance. The default kubevirt console does not take advantage of the GPU. For that we need to take one final step to use the Windows Remote Desktop Protocol (RDP) which can use the GPU.&lt;/p&gt;

&lt;h2 id=&quot;using-the-gpu&quot;&gt;Using the GPU&lt;/h2&gt;

&lt;p&gt;In order to take advantage of the virtual GPU we have added, we will need to connect to the virtual machine over Remote Desktop Protocol (RDP). Follow these steps to enable RDP:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;In the Windows 10 search bar, type ‚Äú&lt;strong&gt;Remote Desktop Settings&lt;/strong&gt;‚Äù and then open the result.&lt;/li&gt;
  &lt;li&gt;Select ‚Äú&lt;strong&gt;Enable Remote Desktop&lt;/strong&gt;‚Äù and confirm the change.&lt;/li&gt;
  &lt;li&gt;Select ‚Äú&lt;strong&gt;Advanced settings&lt;/strong&gt;‚Äù and un-check ‚Äú&lt;strong&gt;Require computers to use Network level Authentication&lt;/strong&gt;‚Äù, and confirm this change.&lt;/li&gt;
  &lt;li&gt;Finally reboot the Windows 10 Virtual machine.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Now, run the following commands in order to expose the RDP server to outside your Kubernetes cluster:&lt;/p&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;virtctl expose vm win10vm1 &lt;span class=&quot;nt&quot;&gt;--port&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;3389 &lt;span class=&quot;nt&quot;&gt;--type&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;NodePort &lt;span class=&quot;nt&quot;&gt;--name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;win10vm1-rdp
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl get svc
NAME           TYPE        CLUSTER-IP       EXTERNAL-IP   PORT&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;S&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;          AGE
kubernetes     ClusterIP   10.96.0.1        &amp;lt;none&amp;gt;        443/TCP          18h
win10vm1-rdp   NodePort    10.105.159.184   &amp;lt;none&amp;gt;        3389:30627/TCP   39s
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Note the port that was assigned to this service we will use it in the next step. In the above output the port is 30627.&lt;/p&gt;

&lt;p&gt;We can now use the rdesktop tool to connect to our VM and get the full advantages of the vGPU. From a command line run &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;rdesktop localhost:&amp;lt;port&amp;gt;&lt;/code&gt; being sure to update the port based on the output from above. When prompted by rdesktop accept the certificate. Log into your Windows 10 client. You can now test out the vGPU.&lt;/p&gt;

&lt;p&gt;Let‚Äôs try FishGL again. Open a browser and go to &lt;a href=&quot;https://webglsamples.org/fishtank/fishtank.html&quot;&gt;https://webglsamples.org/fishtank/fishtank.html&lt;/a&gt;. You should notice a large improvement in the applications performance. You can also open the Task Manager and look at the performance tab to see the GPU under load.
&lt;br /&gt;&lt;/p&gt;

&lt;div class=&quot;my-gallery&quot; itemscope=&quot;&quot; itemtype=&quot;http://schema.org/ImageGallery&quot;&gt;
  &lt;figure itemprop=&quot;associatedMedia&quot; itemscope=&quot;&quot; itemtype=&quot;http://schema.org/ImageObject&quot;&gt;
    &lt;a href=&quot;/assets/2021-04-30-intel-vgpu-kubevirt/fishgl-gpu.png&quot; itemprop=&quot;contentUrl&quot; data-size=&quot;800x530&quot;&gt;
      &lt;img src=&quot;/assets/2021-04-30-intel-vgpu-kubevirt/fishgl-gpu.png&quot; itemprop=&quot;thumbnail&quot; width=&quot;100%&quot; alt=&quot;FishGL&quot; /&gt;
    &lt;/a&gt;
    &lt;figcaption itemprop=&quot;caption description&quot;&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Note that since you are running your Fedora 32 workstation on this same GPU you are already sharing the graphics workload between your primary desktop, and the virtualized Windows Desktop also running on this machine.&lt;/p&gt;

&lt;p&gt;Congratulations! You now have a VM running in Kubernetes using an Intel vGPU. If your test machine has enough resources you can repeat the steps and create multiple virtual machines all sharing the one Intel GPU.&lt;/p&gt;</content><author><name>Mark DeNeve</name></author><category term="news" /><category term="kubevirt" /><category term="vGPU" /><category term="Windows" /><category term="GPU" /><category term="Intel" /><category term="minikube" /><category term="Fedora" /><summary type="html">Introduction Prerequisites Fedora Workstation Prep Preparing the Intel vGPU driver Install Kubernetes with minikube Install kubevirt Validate vGPU detection Install Containerize Data Importer Install Windows Accessing the Windows VM Using the GPU</summary></entry></feed>